<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Introduction to Netidx</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="quick_start.html"><strong aria-hidden="true">1.</strong> Quick Start</a></li><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">2.</strong> Overview</a></li><li class="chapter-item expanded "><a href="examples_overview.html"><strong aria-hidden="true">3.</strong> Examples</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="vmstat_example.html"><strong aria-hidden="true">3.1.</strong> Publishing vmstat</a></li><li class="chapter-item expanded "><a href="integration_example.html"><strong aria-hidden="true">3.2.</strong> Integration into an Existing System</a></li><li class="chapter-item expanded "><a href="complete_system.html"><strong aria-hidden="true">3.3.</strong> Clean Slate System Design</a></li></ol></li><li class="chapter-item expanded "><a href="administration.html"><strong aria-hidden="true">4.</strong> Administration</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="configuration.html"><strong aria-hidden="true">4.1.</strong> Configuration</a></li><li class="chapter-item expanded "><a href="authorization.html"><strong aria-hidden="true">4.2.</strong> Authorization</a></li><li class="chapter-item expanded "><a href="startup.html"><strong aria-hidden="true">4.3.</strong> Running the Resolver Server</a></li><li class="chapter-item expanded "><a href="subscription_flow.html"><strong aria-hidden="true">4.4.</strong> Subscription Flow</a></li><li class="chapter-item expanded "><a href="fault_tolerance.html"><strong aria-hidden="true">4.5.</strong> Fault Tolerance</a></li></ol></li><li class="chapter-item expanded "><a href="tools_overview.html"><strong aria-hidden="true">5.</strong> Command Line Tools</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="publisher_tool.html"><strong aria-hidden="true">5.1.</strong> publisher</a></li><li class="chapter-item expanded "><a href="subscriber_tool.html"><strong aria-hidden="true">5.2.</strong> subscriber</a></li><li class="chapter-item expanded "><a href="resolver_tool.html"><strong aria-hidden="true">5.3.</strong> resolver</a></li><li class="chapter-item expanded "><a href="recorder_tool.html"><strong aria-hidden="true">5.4.</strong> recorder</a></li><li class="chapter-item expanded "><a href="stress_tool.html"><strong aria-hidden="true">5.5.</strong> stress</a></li></ol></li><li class="chapter-item expanded "><a href="browser_overview.html"><strong aria-hidden="true">6.</strong> Browser</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="browser_views.html"><strong aria-hidden="true">6.1.</strong> Constructing Custom Views</a></li><li class="chapter-item expanded "><a href="browser_scripting.html"><strong aria-hidden="true">6.2.</strong> BScript</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">Introduction to Netidx</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1><a class="header" href="#quick-start-for-linux" id="quick-start-for-linux">Quick Start for Linux</a></h1>
<p>Install <a href="https://www.rust-lang.org/tools/install">rust</a> via rustup if
you haven't already. Ensure <code>~/.cargo/bin</code> is in your PATH.</p>
<p><code>cargo install netidx-tools</code></p>
<p>This will build and install the <code>netidx</code> command, which contains all
the built in command line tools necessary to run to the resolver
server, as well as the publisher/subscriber command line tools</p>
<p>You will need some build dependencies,</p>
<ul>
<li>libclang, necessary for bindgen, on debian/ubuntu <code>sudo apt install libclang-dev</code></li>
<li>gssapi, necessary for kerberos support, on debian/ubuntu <code>sudo apt install libkrb5-dev</code></li>
</ul>
<pre><code>{
    &quot;parent&quot;: null,
    &quot;children&quot;: [],
    &quot;pid_file&quot;: &quot;&quot;,
    &quot;addrs&quot;: [&quot;127.0.0.1:4564&quot;],
    &quot;max_connections&quot;: 512,
    &quot;hello_timeout&quot;: 10,
    &quot;reader_ttl&quot;: 60,
    &quot;writer_ttl&quot;: 120,
    &quot;auth&quot;: &quot;Anonymous&quot;
}
</code></pre>
<p>Install the above config in <code>~/.config/netidx.json</code>. This config will
only allow communication on your local machine. Make sure port 4564 is
free, or change it to a free port of your choosing.</p>
<p>run <code>netidx resolver-server</code>. This command will return immediatly, and
the resolver server will daemonize. Check that it's running using <code>ps auxwww | grep netidx</code>.</p>
<p>To test the configuration run,</p>
<p><code>netidx stress publisher --bind 127.0.0.1/0 --delay 1000 1000 10</code></p>
<p>This will publish 10,000 items following the pattern <code>/bench/$r/$c</code>
where <code>$r</code> is a row number and <code>$c</code> is a column
number. e.g. <code>/bench/100/8</code> corresponds to row 100 column 8. The
browser will draw this as a table with 1000 rows and 10 columns,
however for this test we will use the command line subscriber to look
at one cell in the table.</p>
<p><code>netidx subscriber /bench/0/0</code></p>
<p>should print out one line like this every second</p>
<p><code>/bench/0/0|v64|1</code></p>
<p>The final number should increment, and if that works then netidx is
set up on your local machine. If it didn't work, try setting the
environment variable <code>RUST_LOG=debug</code> and running the stress publisher
and the subscriber again.</p>
<h2><a class="header" href="#optional-netidx-browser" id="optional-netidx-browser">Optional Netidx Browser</a></h2>
<p>The browser is an optional gui browser for the netidx tree, you need
gtk development files installed to build it, on debian/ubuntu add those with </p>
<p><code>sudo apt install libgtk-3-dev</code></p>
<p>and then</p>
<p><code>cargo install netidx-browser</code></p>
<h1><a class="header" href="#overview-of-netidx" id="overview-of-netidx">Overview of Netidx</a></h1>
<p>Netidx is a library, protocol, and server that facilitates publishing
the value of a variable in one program and consuming it in another
program, possibly on another computer. There are a lot of details, but
making that transaction as easy as possible, while still being secure
and performant is the essential goal.</p>
<h2><a class="header" href="#the-namespace" id="the-namespace">The Namespace</a></h2>
<p>Netidx values are published to a hierarchical tuple space. The
structure of the names look just like a filename, e.g.</p>
<pre><code>/apps/solar/stats/battery_sense_voltage
</code></pre>
<p>Is an example name. Unlike a file name, a netidx name may point to a
value, and also have children. So keeping the file analogy, it can be
both a file and a directory. For example we might have,</p>
<pre><code>/apps/solar/stats/battery_sense_voltage/millivolts
</code></pre>
<p>Where the <code>.../battery_sense_voltage</code> points to the number in volts,
and it's 'millivolts' child gives the same number in millivolts.</p>
<p>Sometimes a name like <code>battery_sense_voltage</code> is published deep in the
hierarchy and it's parents are just structure. Unlike the file system
the resolver server will create and delete those structural containers
automatically, there is no need to manually manage them.</p>
<p>The term 'points to' is literal. In netidx the actual data is
completely separate from the names. The names are stored in the
resolver server cluster. Each name points to the ip address and port
of the publisher that actually has the data.</p>
<p>When a client wants to subscribe to the value pointed to by a name, it
queries the resolver server cluster, and is given the addresses of all
the publishers that publish said data point. It then randomly permutes
that list, and tries to subscribe to each address. If one of them
succeeds, then the subscription succeeds, if they all fail then it
doesn't. All the actual data flows from publishers to subscribers
directly without ever going through any kind of centralized
infrastructure.</p>
<h2><a class="header" href="#whats-a-value" id="whats-a-value">What's a Value</a></h2>
<p>Values are primitives, e.g. various kinds of number, strings,
durations, timestamps, and byte arrays. Values don't have any inherent
structure, but of course you can use byte arrays to publish anything
that can be serialized, and since byte arrays are zero copy that is
even quite efficient.</p>
<p>Published values have some other properties as well,</p>
<ul>
<li>Every non structural name points to a value</li>
<li>Every new subscription immediately delivers it's most recent value</li>
<li>When a value is updated, every subscriber receives the new value</li>
<li>Updates arrive reliably and in the order the publisher made them
(like a TCP stream)</li>
</ul>
<h2><a class="header" href="#scale" id="scale">Scale</a></h2>
<p>Netidx is meant to be a building block, and as such a lot of thought
has gone into scale. There are multiple different parts of the system
that need to scale. The resolver servers, being the only centralized
piece of infrastructure, are perhaps the most important piece, though
the publisher and subscriber also need to be fast or it won't be worth
using.</p>
<h3><a class="header" href="#resolver-server" id="resolver-server">Resolver Server</a></h3>
<p>The resolver servers implement two strategies to achieve
scale. Replication is the first, one can deploy multiple replicas to
multiple machines in order to protect against a single machine outage,
and also increase throughput. In netidx, the publisher itself is the
primary, and as such it is responsible for replicating the names it
publishes out to all the configured resolver servers. This makes the
system very resilient, as even if the entire resolver server cluster
goes down, the data isn't lost if the publishers are still alive. They
will keep trying to republish their data with linear backoff until
they are killed.</p>
<p>Hierarchy is the second scaling strategy. When a system grows too big
to fit in even a large cluster of servers, then busy parts of the
namespace can be delegated to 'child' server clusters. Readers
familiar with DNS will recognize the basic strategy, though the
details not exactly the same. The administration overhead is similarly
hierarchical, since each cluster config file must only know about it's
immediate superior and immediate children. It's entirely possible for
a large organization to run a central 'root' resolver server cluster
without needing to micro manage the delegation going on in various
organizational units.</p>
<p>I've focused on designing a scaleable architecture, but I should also
mention that the resolver server itself is pretty fast, and uses a
number of strategies to minimize memory use. It's entirely possible to
put 100 million names in a single instance on a single machine with
32 - 64 gig of ram. You get roughly 1 million names per 500 MB of ram,
assuming your paths aren't crazy long. I have not explicitly tested
the resolve throughput, but given that it uses the same infrastructure
as the publisher/subscriber (which I have tested), and what it's
doing, I would not be at all surprised if you could support millions
of resolutions per second per core (yes it will use all your cores).</p>
<h3><a class="header" href="#publishersubscriber" id="publishersubscriber">Publisher/Subscriber</a></h3>
<p>On the wire, the netidx protocol is almost exactly the same as
protobuf. In protobuf, each record is extensible and rather cleverly
encoded. Each field in the record has a LEB128 Id, followed by a data
value.</p>
<p>In netidx, the subscriber sends the name it wants to one of the
publishers specified by the resolver server cluster. The publisher
looks up that value, and responds with the id it will use in
subsequent messages, along with the current value. From then on
updates to that value transmit only the id, which is LEB128 encoded,
and the updated value. So on the wire, in terms of overhead, it looks
very much like a protobuf record where the fields are exactly what the
subscriber has requested and nothing more. The overhead of sending an
f64 can be as small as 2 additional bytes (so 10 in total, 1 id byte,
1 tag byte).</p>
<p>Publisher and subscriber performance is fairly good, such that sending
many millions of messages per second is possible. As of this writing a
fast machine can send about 15 million kerberos encrypted
messages/second and more then 20 million in the clear. The per message
overhead is on the order of about 50ns of wall clock time per message
with kerberos encryption on. Obviously that number depends on the
exact hardware you're running on, and it depends on your workload
batching well.</p>
<p>The subscriber library also implements zero copy decoding for strings
and byte arrays, so it is possible to receive large binary encoded
things quite efficiently.</p>
<h2><a class="header" href="#security" id="security">Security</a></h2>
<p>No system like netidx can be taken seriously without a plausible
design for securing data against unauthorized access, interception,
manipulation, etc.</p>
<p>The heart of netidx security is Kerberos v5. There are a lot of
systems I might have used, e.g. openssl + certificates, oauth +
openssl, and I'm sure many others. The reason I chose to use Kerberos
v5 is that most users who I think might want to deploy netidx services
already have Kerberos set up (even if they don't know it) in the form
of Microsoft Active Directory, Samba ADS, Redhat Directory Server, or
one of the many other compatible solutions.</p>
<p>That said security is optional in netidx. It's possible to deploy a
netidx system with no security at all, and it's possible to deploy a
system where some publishers require security, and some do not. While
it's possible to mix secured and non secured publishers on the same
resolver cluster there are some restrictions. </p>
<ul>
<li>If a subscriber is configured with security, then it won't talk to
publishers that aren't.</li>
<li>If a publisher is configured with security, then it won't talk to a
subscriber that isn't.</li>
</ul>
<p>When security is enabled you get the following guarantees,</p>
<ul>
<li>
<p><strong>Mutual Authentication</strong>, the publisher knows the subscriber is who
they claim to be, and the subscriber knows the publisher is who they
claim to be. This applies for the resolver &lt;-&gt; subscriber, and
resolver &lt;-&gt; publisher as well.</p>
</li>
<li>
<p><strong>Confidentiality</strong> and Tamper detection, all messages are encrypted,
and data cannot be altered undetected by a man in the middle.</p>
</li>
<li>
<p><strong>Authorization</strong>, The user subscribing to a given data value is
authorized to do so. The resolver servers maintain a permissions
database specifying who is allowed to do what where in the
tree. Thus the system administrator can centrally control who is
allowed to publish and subscribe where.</p>
</li>
</ul>
<h2><a class="header" href="#cross-platform" id="cross-platform">Cross Platform</a></h2>
<p>While netidx is primarily developed on Linux, it has been tested on
Windows, and even Mac OS. It will probably work on many platforms I
haven't tried.</p>
<h1><a class="header" href="#examples" id="examples">Examples</a></h1>
<p>In this chapter we present three examples ranging in complexity from
some shell scripts, to a clean slate design of a custom embedded
application.</p>
<h1><a class="header" href="#publishing-vmstat" id="publishing-vmstat">Publishing vmstat</a></h1>
<p>In this example we build a shell script to publish the output of the
venerable vmstat tool to netidx.</p>
<pre><code class="language-bash">#! /bin/bash

BASE=&quot;/sys/vmstat/$HOSTNAME&quot;

vmstat -n 1 | \
    while read running \
               blocked \
               swapped \
               free \
               buf \
               cache \
               swap_in \
               swap_out \
               blocks_in \
               blocks_out \
               interrupts \
               context_switches \
               user \
               system \
               idle \
               waitio \
               stolen
    do
        echo &quot;${BASE}/running|z32|${running}&quot;
        echo &quot;${BASE}/blocked|z32|${blocked}&quot;
        echo &quot;${BASE}/swapped|z32|${swapped}&quot;
        echo &quot;${BASE}/free|u64|${free}&quot;
        echo &quot;${BASE}/buf|u64|${buf}&quot;
        echo &quot;${BASE}/cache|u64|${cache}&quot;
        echo &quot;${BASE}/swap_in|z32|${swap_in}&quot;
        echo &quot;${BASE}/swap_out|z32|${swap_out}&quot;
        echo &quot;${BASE}/blocks_in|z32|${blocks_in}&quot;
        echo &quot;${BASE}/blocks_out|z32|${blocks_out}&quot;
        echo &quot;${BASE}/interrupts|z32|${interrupts}&quot;
        echo &quot;${BASE}/context_switches|z32|${context_switches}&quot;
        echo &quot;${BASE}/user|z32|${user}&quot;
        echo &quot;${BASE}/system|z32|${system}&quot;
        echo &quot;${BASE}/idle|z32|${idle}&quot;
        echo &quot;${BASE}/waitio|z32|${waitio}&quot;
        echo &quot;${BASE}/stolen|z32|${stolen}&quot;
    done | \
    netidx publisher --spn publish/${HOSTNAME}@RYU-OH.ORG --bind 192.168.0.0/24
</code></pre>
<p>Lets dissect this pipeline of three commands into it's parts. First
vmstat itself, if you aren't familiar with it, is part of the <code>procps</code>
package on debian, which is a set of fundamental unix tools like
<code>pkill</code>, <code>free</code>, and <code>w</code> which would have been familiar to sysadmins
in the 80s.</p>
<pre><code>vmstat -n 1
</code></pre>
<p>prints output like this</p>
<pre><code>eric@ken-ohki:~$ vmstat -n 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0 279988 4607136 1194752 19779448    0    0     5    48   14    8 15  6 79  0  0
 0  0 279988 4605104 1194752 19780728    0    0     0     0 1800 3869  4  1 95  0  0
 0  0 279988 4605372 1194752 19780632    0    0     0     0 1797 4175  3  1 96  0  0
 0  0 279988 4604104 1194752 19781672    0    0     0     0 1982 4570  4  1 95  0  0
 0  0 279988 4604112 1194752 19780648    0    0     0     0 1941 4690  3  2 95  0  0
</code></pre>
<p><code>-n</code> means only print the header once, and <code>1</code> means print a line
every second until killed. Next we pass these lines to a shell while
loop that reads the line using the builtin <code>read</code> command into a shell
variable for each field. I've given the fields more verbose names than
vmstat did. In the body of the while loop I echo a <code>path|typ|value</code>
triple for each field. e.g. if we don't run the final pipe to <code>netidx publisher</code> the output of the while loop looks something like this.</p>
<pre><code>/sys/vmstat/ken-ohki.ryu-oh.org/running|z32|1
/sys/vmstat/ken-ohki.ryu-oh.org/blocked|z32|0
/sys/vmstat/ken-ohki.ryu-oh.org/swapped|z32|279988
/sys/vmstat/ken-ohki.ryu-oh.org/free|u64|4644952
/sys/vmstat/ken-ohki.ryu-oh.org/buf|u64|1194896
/sys/vmstat/ken-ohki.ryu-oh.org/cache|u64|19775864
/sys/vmstat/ken-ohki.ryu-oh.org/swap_in|z32|0
/sys/vmstat/ken-ohki.ryu-oh.org/swap_out|z32|0
/sys/vmstat/ken-ohki.ryu-oh.org/blocks_in|z32|5
/sys/vmstat/ken-ohki.ryu-oh.org/blocks_out|z32|48
/sys/vmstat/ken-ohki.ryu-oh.org/interrupts|z32|14
/sys/vmstat/ken-ohki.ryu-oh.org/context_switches|z32|9
/sys/vmstat/ken-ohki.ryu-oh.org/user|z32|15
/sys/vmstat/ken-ohki.ryu-oh.org/system|z32|6
/sys/vmstat/ken-ohki.ryu-oh.org/idle|z32|79
/sys/vmstat/ken-ohki.ryu-oh.org/waitio|z32|0
/sys/vmstat/ken-ohki.ryu-oh.org/stolen|z32|0
</code></pre>
<p>No surprise that this is the exact format <code>netidx publisher</code> requires
to publish a value, so the final command in the pipeline is just
<code>netidx publisher</code> consuming the output of the while loop. </p>
<p>Running this on two of my systems results in a table viewable in the
browser with two rows,</p>
<p><img src="vmstat-browser-table.png" alt="vmstat" /></p>
<p>Because of the way the browser works, our regular tree structure is
automatically turned into a table with a row for each host, and a
column for each vmstat field. So we've made something that's
potentially useful to look at with very little effort. There are many
other things we can now do with this data, for example we could use
<code>netidx record</code> to record the history of vmstat on these machines, we
could subscribe, compute an aggregate, and republish it, or we could
sort by various columns in the browser. How about we have some fun and
pretend that all the machines running the script are part of an
integrated cluster, as if it was that easy, and so we want a total
vmstat for the cluster.</p>
<pre><code class="language-bash">#! /bin/bash

BASE='/sys/vmstat'
declare -A TOTALS
declare -A HOSTS

netidx resolver list -w &quot;${BASE}/**&quot; | \
    grep -v --line-buffered &quot;${BASE}/total/&quot; | \
    sed -u -e 's/^/ADD|/' | \
    netidx subscriber | \
    while IFS='|' read -a input
    do
        IFS='/' path=(${input[0]})
        host=${path[-2]}
        field=${path[-1]}
        if ! test -z &quot;$host&quot; -o -z &quot;$field&quot;; then
            HOSTS[$host]=&quot;$host&quot;
            TOTALS[&quot;$host/$field&quot;]=${input[2]}
            T=0
            for h in ${HOSTS[@]}
            do
                ((T+=TOTALS[&quot;$h/$field&quot;]))
            done
            echo &quot;${BASE}/total/$field|${input[1]}|$T&quot;
        fi
    done | netidx publisher --spn publish/${HOSTNAME}@RYU-OH.ORG --bind 192.168.0.0/24
</code></pre>
<p>Lets dissect this script,</p>
<pre><code>netidx resolve list -w &quot;${BASE}/**&quot;
</code></pre>
<p>This lists everything under <code>/sys/vmstat</code> recursively, and instead of
exiting after doing that, it keeps polling every second, and if a new
thing shows up that matches the glob it lists the new thing. The
output is just a list of paths, e.g.</p>
<pre><code>...
/sys/vmstat/total/blocked
/sys/vmstat/total/buf
/sys/vmstat/total/system
/sys/vmstat/blackbird.ryu-oh.org/swap_out
/sys/vmstat/ken-ohki.ryu-oh.org/buf
/sys/vmstat/ken-ohki.ryu-oh.org/idle
...
</code></pre>
<p>The next two commands in the pipeline serve to filter out the total
row we are going to publish (don't want to recursively total things
right), and transform the remaining lines into commands to <code>netidx subscriber</code> that will cause it to add a subscription. e.g.</p>
<pre><code>...
ADD|/sys/vmstat/ken-ohki.ryu-oh.org/waitio
ADD|/sys/vmstat/blackbird.ryu-oh.org/blocked
ADD|/sys/vmstat/blackbird.ryu-oh.org/context_switches
ADD|/sys/vmstat/blackbird.ryu-oh.org/free
...
</code></pre>
<p>The above is what gets fed into the <code>netidx subscriber</code> command. So in
a nutshell we've said subscribe to all the things anywhere under
<code>/sys/vmstat</code> that are present now, or appear in the future, and
aren't part of the total row. Subscriber prints a line for each
subscription update in the form of a <code>PATH|TYP|VAL</code> triple, e.g.</p>
<pre><code>..
/sys/vmstat/ken-ohki.ryu-oh.org/swap_out|z32|0
/sys/vmstat/ken-ohki.ryu-oh.org/blocks_in|z32|0
/sys/vmstat/ken-ohki.ryu-oh.org/blocks_out|z32|16
/sys/vmstat/ken-ohki.ryu-oh.org/interrupts|z32|1169
/sys/vmstat/ken-ohki.ryu-oh.org/context_switches|z32|3710
/sys/vmstat/ken-ohki.ryu-oh.org/user|z32|3
/sys/vmstat/ken-ohki.ryu-oh.org/system|z32|1
/sys/vmstat/ken-ohki.ryu-oh.org/idle|z32|96
...
</code></pre>
<p>That gets passed into our big shell while loop, which uses the <code>read</code>
builtin to read each line into an array called input. So in the body
of each iteration of the the while loop the variable <code>input</code> will be
an array with contents e.g.</p>
<pre><code>[/sys/vmstat/ken-ohki.ryu-oh.org/swap_out, v32, 25]
</code></pre>
<p>Indexed starting at 0 as is the convention in bash. We split the path
into an array called <code>path</code>, the last two elements of which are
important to us. The last element is the field (e.g. <code>swap_out</code>), and
the second to last is the host. Each line is an update to one field of
one host, and when a field of a host is updated we want to compute the
sum of that field for all the hosts, and then print the new total for
that field. To do this we need to remember each field for each host,
since only one field of one host gets updated at a time. For this we
use an associative array with a key of <code>$host/$field</code>, thats
<code>TOTALS</code>. We also need to remember all the host names, so that when we
are ready to compute our total, we can look up the field for every
host, that's <code>HOSTS</code>. Finally we pass the output of this while loop to
the publisher, and now we have a published total row.</p>
<h1><a class="header" href="#integration-into-an-existing-system" id="integration-into-an-existing-system">Integration Into an Existing System</a></h1>
<p>Suppose we have a small daemon that we run on many computers on our
network, and it knows many things about them, and does many things. I
won't specify exactly what it does or everything it knows because
that's irrelevant to the example. However suppose one of the things it
knows is the current CPU temperature of the machine it's running on,
and we would like access to that data. We heard about this new netidx
thing, and we'd like to try it out on this small and not very
important case. What code do we need to add to our daemon, and what
options do we have for using the data?</p>
<p>We can modify our Cargo.toml to include netidx, and then add a small
self contained module, publisher.rs</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use anyhow::Result;
use netidx::{
    config::Config,
    path::Path,
    publisher::{Publisher, Val, Value},
    resolver::Auth,
};

#[derive(Clone)]
pub struct HwPub {
    publisher: Publisher,
    cpu_temp: Val,
}

impl HwPub {
    pub async fn new(host: &amp;str, current: f64) -&gt; Result&lt;HwPub&gt; {
        // load the site cluster config from the path in the
        // environment variable NETIDX_CFG, or from
        // dirs::config_dir()/netidx.json if the environment variable
        // isn't specified, or from ~/.netidx.json if the previous
        // file isn't present. Note this uses the cross platform dirs
        // library, so yes, it does something reasonable on windows.
        let cfg = Config::load_default()?;

        // for this small service we don't need authentication
        let auth = Auth::Anonymous;

        // listen on any unique address matching 192.168.0.0/24. If
        // our network was large and complex we might need to make
        // this a passed in configuration option, but lets assume it's
        // simple.
        let publisher = Publisher::new(cfg, auth, &quot;192.168.0.0/24&quot;.parse()?).await?;

        // We're publishing stats about hardware here, so lets put it
        // in /hw/hostname/cpu-temp, that way we keep everything nice
        // and organized.
        let path = Path::from(format!(&quot;/hw/{}/cpu-temp&quot;, host));
        let cpu_temp = publisher.publish(path, Value::F64(current))?;
        // flush our publish request to the resolver server. Nothing
        // happens until you do this.
        publisher.flush(None).await;
        Ok(HwPub {
            publisher,
            cpu_temp,
        })
    }

    pub async fn update(&amp;self, current: f64) {
        // update the current cpu-temp
        self.cpu_temp.update(Value::F64(current));

        // flush the updated values out to subscribers
        self.publisher.flush(None).await
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>Now all we would need to do is create a HwPub on startup, and call
HwPub::update whenever we learn about a new cpu temperature value. Of
course we also need to deploy a resolver server, and distribute a
cluster config to each machine that needs one, that will be covered in
the administration section.</p>
<h2><a class="header" href="#using-the-data-we-just-published" id="using-the-data-we-just-published">Using the Data We Just Published</a></h2>
<p>So now that we have our data in netidx, what are our options for
consuming it? The first option, and often a very good one for a lot of
applications is the shell. The netidx command line tools are designed
to make this interaction easy, here's an example of how we might use
the data.</p>
<pre><code class="language-bash">#! /bin/bash

netidx subscriber $(netidx resolver list '/hw/*/cpu-temp') | \
while IFS='|' read path typ temp; do
    IFS='/' pparts=($path)
    if ((temp &gt; 75)); then
        echo &quot;host: ${pparts[2]} cpu tmp is too high: ${temp}&quot;
    fi
done
</code></pre>
<p>Of course we can hook any logic we want into this, the shell is a very
powerful tool after all. For example one thing we might want do is
modify this script slightly, filter the entries with cpu temps that
are too high, and then publish the temperature and the timestamp when
it was observed.</p>
<pre><code class="language-bash">#! /bin/bash

netidx subscriber $(netidx resolver list '/hw/*/cpu-temp') | \
while IFS='|' read path typ temp; do
    IFS='/' pparts=($path)
    if ((temp &gt; 75)); then
        echo &quot;/hw/${pparts[2]}/overtemp-ts|string|$(date)&quot;
        echo &quot;/hw/${pparts[2]}/overtemp|f64|$temp&quot;
    fi
done | \
netidx publisher --bind 192.168.0.0/24
</code></pre>
<p>Now we've done something very interesting, we took some data out of
netidx, did a computation on it, and published the result into the
same namespace. We can now subscribe to e.g. <code>/hw/krusty/overtemp-ts</code>
and we will know when that machine last went over temperature. To a
user looking at this namespace in the browser (more on that later)
there is no indication that the over temp data comes from a separate
process, on a separate machine, written by a separate person. It all
just fits together seamlessly as if it was one application.</p>
<p>There is actually a problem here, in that, the above code will not do
quite what you might want it to do. Someone might, for example, want
to write the following additional script.</p>
<pre><code class="language-bash">#! /bin/bash

netidx subscriber $(netidx resolver list '/hw/*/overtemp-ts') | \
while IFS='|' read path typ temp; do
    IFS='/' pparts=($path)
    ring-very-loud-alarm ${pparts[2]}
done
</code></pre>
<p>To ring a very loud alarm when an over temp event is detected. This
would in fact work, it just would not be as timely as the author might
expect. The reason is that the subscriber practices linear backoff
when it's instructed to subscribe to a path that doesn't exist. This
is a good practice, in general it reduces the cost of mistakes on the
entire system, but in this case it could result in getting the alarm
minutes, hours, or longer after you should. The good news is there is
a simple solution, we just need to publish all the paths from the
start, but fill them will null until the event actually happens (and
change the above code to ignore the null). That way the subscription
will be successful right away, and the alarm will sound immediately
after the event is detected. So lets change the code ...</p>
<pre><code class="language-bash">#! /bin/bash

declare -A HOSTS
netidx resolver list -w '/hw/*/cpu-temp' | \
    sed -u -e 's/^/ADD|/' | \
    netidx subscriber | \
    while IFS='|' read path typ temp
    do
        IFS='/' pparts=($path)
        temp=$(sed -e 's/\.[0-9]*//' &lt;&lt;&lt; &quot;$temp&quot;) # strip the fractional part, if any
        host=${pparts[2]}
        if ((temp &gt; 75)); then
            HOSTS[$host]=$host
            echo &quot;/hw/${host}/overtemp-ts|string|$(date)&quot;
            echo &quot;/hw/${host}/overtemp|f64|$temp&quot;
        elif test -z &quot;${HOSTS[$host]}&quot;; then
            HOSTS[$host]=$host
            echo &quot;/hw/${host}/overtemp-ts|null&quot;
            echo &quot;/hw/${host}/overtemp|null&quot;
        fi
    done | netidx publisher --bind 192.168.0.0/24
</code></pre>
<p>We use <code>resolver list -w</code> to list all paths that match
<code>/hw/*/cpu-temp</code>, and watch for new ones that might appear later. We
take that output, which is just a list of paths, and use sed to
prepend <code>ADD|</code> to it, which makes it a valid subscribe request for
<code>netidx subscriber</code>. We then process the resulting cpu temperature
records. We check for over temp, and we store each host in an
associative array. If this is the first time we've seen a given host,
then we set it's initial <code>overtemp-ts</code> and <code>overtemp</code> to null,
otherwise we don't do anything unless it's actually too hot. Even
though it's only a little longer, this shell program has a number of
advantages over the previous version.</p>
<ul>
<li>It will automatically start checking the cpu temp of new hosts as
they are added</li>
<li>It will always publish a row for every host, but will fill it with
null if it has never seen that host over temp. This allows clients
to subscribe to the overtemp value and receive a timely notification
when a host goes over temperature, and it's also nicer to look at in
the browser.</li>
<li>It handles the fractional part of the temperature properly for the
shell, which can't do floating point math (in this case we don't
care)</li>
</ul>
<h2><a class="header" href="#or-maybe-shell-is-not-your-jam" id="or-maybe-shell-is-not-your-jam">Or Maybe Shell is Not Your Jam</a></h2>
<p>It's entirely possible that thinking about the above solution makes
you shiver and reinforces for you that nothing should ever be written
in shell. In that case it's perfectly possible to do the same thing in
rust.</p>
<pre><pre class="playground"><code class="language-rust">use anyhow::Result;
use chrono::prelude::*;
use futures::{
    channel::mpsc::{channel, Sender},
    prelude::*,
};
use netidx::{
    chars::Chars,
    config::Config,
    path::Path,
    pool::Pooled,
    publisher::{self, Publisher, Value},
    resolver::{Auth, ChangeTracker, Glob, GlobSet},
    subscriber::{self, Event, SubId, Subscriber, UpdatesFlags},
};
use std::{
    collections::{HashMap, HashSet},
    iter,
    sync::{Arc, Mutex},
    time::Duration,
};
use tokio::{task, time};

struct Temp {
    // we need to hang onto this reference to keep the subscription alive
    _current: subscriber::Dval, 
    timestamp: publisher::Val,
    temperature: publisher::Val,
}

async fn watch_hosts(
    subscriber: Subscriber,
    publisher: Publisher,
    tx_current: Sender&lt;Pooled&lt;Vec&lt;(SubId, Event)&gt;&gt;&gt;,
    temps: Arc&lt;Mutex&lt;HashMap&lt;SubId, Temp&gt;&gt;&gt;,
) -&gt; Result&lt;()&gt; {
    // we keep track of all the hosts we've already seen, so we don't
    // publish an overtemp record for any host more than once.
    let mut all_hosts = HashSet::new();
    // we will talk directly to the resolver server cluster. The
    // ChangeTracker will allow us to efficiently ask if anything new
    // has been published under the /hw subtree. If anything has, then
    // we will list everything in that subtree that matches the glob
    // /hw/*/cpu-temp.
    let resolver = subscriber.resolver();
    let mut ct = ChangeTracker::new(Path::from(&quot;/hw&quot;));
    let pat = GlobSet::new(true, iter::once(Glob::new(Chars::from(&quot;/hw/*/cpu-temp&quot;))?))?;
    loop {
        if resolver.check_changed(&amp;mut ct).await? {
            let mut batches = resolver.list_matching(&amp;pat).await?;
            for mut batch in batches.drain(..) {
                for path in batch.drain(..) {
                    if let Some(host) = path.split('/').nth(2).map(String::from) {
                        if !all_hosts.contains(&amp;host) {
                            // lock the temps table now to ensure the
                            // main loop can't see an update for an
                            // entry that isn't there yet.
                            let mut temps = temps.lock().unwrap();
                            // subscribe and register to receive updates
                            let current = subscriber.durable_subscribe(path.clone());
                            current.updates(
                                UpdatesFlags::BEGIN_WITH_LAST,
                                tx_current.clone(),
                            );
                            // publish the overtemp records, both with
                            // initial values of Null
                            let timestamp = publisher.publish(
                                Path::from(format!(&quot;/hw/{}/overtemp-ts&quot;, host)),
                                Value::Null,
                            )?;
                            let temperature = publisher.publish(
                                Path::from(format!(&quot;/hw/{}/overtemp&quot;, host)),
                                Value::Null,
                            )?;
                            // record that we've seen the host, and
                            // add the published overtemp record to
                            // the temps table.
                            all_hosts.insert(host);
                            temps.insert(
                                current.id(),
                                Temp { _current: current, timestamp, temperature },
                            );
                        }
                    }
                }
            }
        }
        // flush anything new we've published to the resolver server
        publisher.flush(None).await;
        // wait 1 second before polling the resolver server again
        time::sleep(Duration::from_secs(1)).await
    }
}

#[tokio::main]
pub async fn main() -&gt; Result&lt;()&gt; {
    // load the default netidx config
    let config = Config::load_default()?;
    let auth = Auth::Anonymous;
    // setup subscriber and publisher
    let subscriber = Subscriber::new(config.clone(), auth.clone())?;
    let publisher = Publisher::new(config, auth, &quot;192.168.0.0/24&quot;.parse()?).await?;
    let (tx_current, mut rx_current) = channel(3);
    // this is where we'll store our published overtemp record for each host
    let temps: Arc&lt;Mutex&lt;HashMap&lt;SubId, Temp&gt;&gt;&gt; = Arc::new(Mutex::new(HashMap::new()));
    // start an async task to watch for new hosts publishing cpu-temp records
    task::spawn(watch_hosts(
        subscriber.clone(),
        publisher.clone(),
        tx_current.clone(),
        temps.clone(),
    ));
    while let Some(mut batch) = rx_current.next().await {
        {
            let temps = temps.lock().unwrap();
            for (id, ev) in batch.drain(..) {
                match ev {
                    Event::Unsubscribed =&gt; (), // Subscriber will resubscribe automatically
                    Event::Update(v) =&gt; {
                        if let Ok(temp) = v.cast_to::&lt;f64&gt;() {
                            if temp &gt; 75. {
                                let tr = &amp;temps[&amp;id];
                                tr.timestamp.update(Value::DateTime(Utc::now()));
                                tr.temperature.update(Value::F64(temp));
                            }
                        }
                    }
                }
            }
        } // release the lock before we do any async operations
        publisher.flush(None).await
    }
    Ok(())
}
</code></pre></pre>
<p>This does almost exactly the same thing as the shell script, the only
semantic difference being that it sends an actual DateTime value for
the timestamp instead of a string, which would certainly make life
easier for anyone using this data, not to mention it's more
efficient.</p>
<h2><a class="header" href="#but-i-just-want-to-look-at-my-data" id="but-i-just-want-to-look-at-my-data">But I Just Want to Look at My Data</a></h2>
<p>Up to now we've covered using the data in various kinds of programs,
but what if you just want to look at it. For that you have two
choices, you can write a custom tool that presents your data exactly
the way you want, or you can use the netidx browser. A custom tool
will always give you more control, but the browser is designed to be
pretty flexible, and it allows you to get to an ok looking solution
really fast. In the case of the data we've been discussing in this
chapter, you get something pretty nice to look at without doing
anything at all.</p>
<p><img src="small-example-table.png" alt="The Browser rendering a table" /></p>
<p>So what's going on here, how did we get a nice looking table out of a
tree? When asked to navigate to a path the browser looks for two kinds
of regular structures, and will draw something appropriate based on
it's findings. One kind is a tree where the 1st level children
themselves have a regular set of children. By regular I mean, with the
same name. In the example we have</p>
<pre><code>/hw/um01-ta07-09/cpu-temp
/hw/um01-ta07-09/overtemp-ts
/hw/um01-ta07-09/overtemp
</code></pre>
<p>But all the 1st level nodes have the same children, so the pattern is,</p>
<pre><code>/hw/${host}/cpu-temp
/hw/${host}/overtemp-ts
/hw/${host}/overtemp
</code></pre>
<p>The browser discovers that regularity, and elects to make a row for
each $host, and a column for each child of $host. In our case, the
data is perfectly regular, and so we end up with a fully populated
table with 3 columns, and a row for each host.</p>
<h1><a class="header" href="#a-clean-slate-design-using-netidx" id="a-clean-slate-design-using-netidx">A Clean Slate Design Using Netidx</a></h1>
<p>In the last chapter we added netidx publishing of one data point from
an existing system, and then explored what we could do with the
data. In this chapter we're going to look at a system designed from
scratch to use netidx as it's primary means of communication and
control.</p>
<p>The system we're going to look at is the control program of an off the
grid solar generator. The system uses a Prostar MPPT controller, which
has a serial port over which it talks modbus. Connected to this port
is raspberry pi 3, called &quot;solar&quot;, which is connected to wifi and
joined to samba ADS.</p>
<p>The control program is a simple translation layer between the modbus
interface of the Prostar and netidx. Full source code
<a href="https://github.com/estokes/solar">here</a>.</p>
<p>The main loop takes commands from either the local command socket, or
the netidx publisher, and sends them via modbus to the charge
controller, e.g.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    loop {
        ...
        match msg {
            ToMainLoop::FromClient(msg, mut reply) =&gt; match msg {
                FromClient::SetCharging(b) =&gt; {
                    send_reply(mb.write_coil(ps::Coil::ChargeDisconnect, !b).await, reply)
                        .await
                }
                FromClient::SetLoad(b) =&gt; {
                    send_reply(mb.write_coil(ps::Coil::LoadDisconnect, !b).await, reply)
                        .await
                }
                FromClient::ResetController =&gt; {
                    send_reply(mb.write_coil(ps::Coil::ResetControl, true).await, reply)
                        .await
                }
                ...
<span class="boring">}
</span></code></pre></pre>
<p>A message is either a timer Tick, on which we send out (and log)
updated stats, or an actual command, which we handle individually. The
publisher module is fed a new stats record read from modbus on each
timer tick. e.g.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    fn update(&amp;self, st: &amp;Stats) {
        use chrono::prelude::*;
        self.timestamp
            .update_changed(Value::DateTime(DateTime::&lt;Utc&gt;::from(st.timestamp)));
        self.software_version.update_changed(Value::V32(st.software_version as u32));
        self.battery_voltage_settings_multiplier
            .update(Value::V32(st.battery_voltage_settings_multiplier as u32));
        self.supply_3v3.update_changed(Value::F32(st.supply_3v3.get::&lt;volt&gt;()));
        self.supply_12v.update_changed(Value::F32(st.supply_12v.get::&lt;volt&gt;()));
        self.supply_5v.update_changed(Value::F32(st.supply_5v.get::&lt;volt&gt;()));
        self.gate_drive_voltage
            .update_changed(Value::F32(st.gate_drive_voltage.get::&lt;volt&gt;()));
        self.battery_terminal_voltage
            .update_changed(Value::F32(st.battery_terminal_voltage.get::&lt;volt&gt;()));
    ...
<span class="boring">}
</span></code></pre></pre>
<p>These are all published under <code>/solar/stats</code>, there are a lot of them,
so I won't show them all here, you can read the full source if you're
curious. Essentially it's an infinite loop of read stats from modbus,
update netidx, flush netidx, loop.</p>
<h2><a class="header" href="#what-about-control" id="what-about-control">What About Control</a></h2>
<p>The above handles distributing the stats perfectly well, but for
control we need some way to send commands from the subscriber back to
the publisher, and that's where writes come in. If you've read the api
documentation you might have noticed,</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn write(&amp;self, v: Value)
<span class="boring">}
</span></code></pre></pre>
<p>Continuing with the metaphor of exporting variables to a cross machine
global namespace, it fits perfectly well to imagine that we can write
to those variables as well as read from them, publisher willing.</p>
<p>Our program is going to publish three values for control,
<code>/solar/control/charging</code> (to control whether we are charging the
batteries), <code>/solar/control/load</code> (to control whether the inverter is on
or off), and <code>/solar/control/reset</code> (to trigger a controller
reset). These values will all be boolean, and they will be valid for
both read and write. Here is the full code of the control section,</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct PublishedControl {
    charging: Val,
    load: Val,
    reset: Val,
}

impl PublishedControl {
    fn new(publisher: &amp;Publisher, base: &amp;Path) -&gt; Result&lt;Self&gt; {
        Ok(PublishedControl {
            charging: publisher.publish(base.append(&quot;charging&quot;), Value::Null)?,
            load: publisher.publish(base.append(&quot;load&quot;), Value::Null)?,
            reset: publisher.publish(base.append(&quot;reset&quot;), Value::Null)?,
        })
    }

    fn update(&amp;self, st: &amp;Stats) {
        self.charging.update_changed(match st.charge_state {
            ChargeState::Disconnect | ChargeState::Fault =&gt; Value::False,
            ChargeState::UnknownState(_)
            | ChargeState::Absorption
            | ChargeState::BulkMPPT
            | ChargeState::Equalize
            | ChargeState::Fixed
            | ChargeState::Float
            | ChargeState::Night
            | ChargeState::NightCheck
            | ChargeState::Start
            | ChargeState::Slave =&gt; Value::True,
        });
        self.load.update_changed(match st.load_state {
            LoadState::Disconnect | LoadState::Fault | LoadState::LVD =&gt; Value::False,
            LoadState::LVDWarning
            | LoadState::Normal
            | LoadState::NormalOff
            | LoadState::NotUsed
            | LoadState::Override
            | LoadState::Start
            | LoadState::Unknown(_) =&gt; Value::True,
        });
    }

    fn register_writable(&amp;self, channel: fmpsc::Sender&lt;Pooled&lt;Vec&lt;WriteRequest&gt;&gt;&gt;) {
        self.charging.writes(channel.clone());
        self.load.writes(channel.clone());
        self.reset.writes(channel.clone());
    }

    fn process_writes(&amp;self, mut batch: Pooled&lt;Vec&lt;WriteRequest&gt;&gt;) -&gt; Vec&lt;FromClient&gt; {
        batch
            .drain(..)
            .filter_map(|r| {
                if r.id == self.charging.id() {
                    Some(FromClient::SetCharging(bool!(r)))
                } else if r.id == self.load.id() {
                    Some(FromClient::SetLoad(bool!(r)))
                } else if r.id == self.reset.id() {
                    Some(FromClient::ResetController)
                } else {
                    let m = format!(&quot;control id {:?} not recognized&quot;, r.id);
                    warn!(&quot;{}&quot;, &amp;m);
                    if let Some(reply) = r.send_result {
                        reply.send(Value::Error(Chars::from(m)));
                    }
                    None
                }
            })
            .collect()
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>In process_writes we translate each WriteRequest that is targeted at
one of the published controls into a FromClient message that the main
loop will act on. So from the main loop's perspective it doesn't
matter if a command came from netidx or the local control socket.</p>
<p>Note that it isn't necessary to do any authorization here, the
publisher library has already checked that the resolver server granted
the user making these writes permission to do them, and of course we
can control who is authorized to write in the resolver server
permissions.</p>
<p>For the basic day to day use case, that's all we need on the server
side. The entire daemon uses 6.5 MB of ram, and almost no cpu, it
could certainly run on a smaller device, though we depend on tokio,
which means we at least need a unix like OS under us.</p>
<p>The kerberos configuration for this service is also quite simple,
there is a service principal called svc_solar in samba ADS, and solar
has a keytab installed for it, as well as a cron job that renews it's
TGT every couple of hours.</p>
<h2><a class="header" href="#building-a-custom-gui-with-views" id="building-a-custom-gui-with-views">Building a Custom GUI With Views</a></h2>
<p>What we have is fine as far as it goes, we can view our stats in the
browser, and we can write to the controls using the command line
subscriber. It's fine for scripting, but I'd also like a gui. e.g.</p>
<p><img src="solar-gui.png" alt="Solar GUI" /></p>
<p>This can be built using design mode in the browser, the view can then
be saved to a file or written directly to a netidx path.</p>
<p><img src="browser-design-mode.png" alt="Browser Design Mode" /></p>
<p>See the chapter on custom views for more information.</p>
<h2><a class="header" href="#wrapping-up" id="wrapping-up">Wrapping Up</a></h2>
<p>In this chapter we saw how an application can be designed more or less
from the start to communicate with the outside world using netidx. We
didn't cover the opportunities for scripting our solar installation
now that we can control it using netidx, but we certainly could do any
of the nice things we did in the last chapter. We did show an example
gui, and I want to point out that having a gui in no way alters our
ability to script and manipulate the system programatically. It's
important to recognize that building a bespoke system with a gui as
complex as the one we built AND making it scriptable over the network
in a discoverable, secure, and performant way is not an easy task.
Usually it just isn't worth doing, however by using netidx it was
easy, and that's really the point of the whole thing.</p>
<h1><a class="header" href="#administration" id="administration">Administration</a></h1>
<h2><a class="header" href="#first-things-first" id="first-things-first">First Things First</a></h2>
<p>If you plan to use Kerberos make sure you have it set up properly,
including your KDC, DNS, DHCP, etc. If you need help with kerberos I
suggest the <a href="https://www.oreilly.com/library/view/kerberos-the-definitive/0596004036/">O'REILLY
book</a>. If
you want something free the <a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/">RedHat
documentation</a>
isn't too bad, though it is somewhat specific to their product.</p>
<p>Problems with Kerberos/GSSAPI can often be diagnosed by setting
<code>KRB5_TRACE=/dev/stderr</code>, and/or <code>RUST_LOG=debug</code>. GSSAPI errors can
sometimes be less than helpful, but usually the KRB5_TRACE is more
informative.</p>
<h2><a class="header" href="#resources-and-gotchas" id="resources-and-gotchas">Resources and Gotchas</a></h2>
<ul>
<li>
<p>Expect to use about 500 MiB of ram in the resolver server for every
1 million published values.</p>
</li>
<li>
<p>Both read and write operations will make use of all available
logical processors on the machine. So, in the case you are hitting
performance problems, try allocating more cores before taking more
drastic segmentation steps.</p>
</li>
<li>
<p>Even when the resolvers are very busy they should remain fair. Large
batches of reads or writes are broken into smaller reasonably sized
batches for each logical processor. These batches are then
interleaved pseudo randomly to ensure that neither reads nor writes
are starved.</p>
</li>
<li>
<p>Be mindful of the maximum number of available file descriptors per
process on the resolver server machine when setting
max_connections. You can easily raise this number on modern linux
systems using ulimit.</p>
</li>
<li>
<p>While the resolver server drops idle read client connections fairly
quickly (default 60 seconds), if you have many thousands or tens of
thousands of read clients that want to do a lot of reading
simultaneously then you may need to raise the maximum number of file
descriptors available, and/or deploy additional processes to avoid
file descriptor exhaustion.</p>
</li>
<li>
<p>Some implementations of Krb5/GSSAPI keep a file descriptor open for
every active client/server session, which in our case means every
read client, but also every publisher, connected or not. This has
been fixed in recent versions of MIT Kerberos (but may still
manifest if you are running with KRB5_TRACE). Keep this in mind if
you're seeing file descriptor exhaustion.</p>
</li>
</ul>
<h2><a class="header" href="#configuration" id="configuration">Configuration</a></h2>
<p>The netidx configuration file is the same for all the different
components of the system, resolver, publisher, and subscriber. By
default it is stored,</p>
<ul>
<li>on Linux: ~/.config/netidx.json</li>
<li>on Windows: ~\AppData\Roaming\netidx.json</li>
<li>on MacOS: ~/Library/Application Support/netidx.json</li>
</ul>
<p>Since the dirs crate is used to discover these paths, they are locally
configurable by OS specific means. Everyone who will use netidx needs
access to this file.</p>
<pre><code class="language-json">{
    &quot;parent&quot;: null,
    &quot;children&quot;: [],
    &quot;pid_file&quot;: &quot;&quot;,
    &quot;addrs&quot;: [&quot;192.168.0.1:4564&quot;],
    &quot;max_connections&quot;: 768,
    &quot;hello_timeout&quot;: 10,
    &quot;reader_ttl&quot;: 60,
    &quot;writer_ttl&quot;: 120,
    &quot;auth&quot;: {
        &quot;Krb5&quot;: {&quot;192.168.0.1:4564&quot;: &quot;netidx/washu-chan.ryu-oh.org@RYU-OH.ORG&quot;}
    }
}
</code></pre>
<p>Here's about the simplest possible Kerberos enabled
configuration. I'll go through each field,</p>
<ul>
<li>parent: null unless this server has a parent</li>
<li>children: empty unless this server has children</li>
<li>pid_file: prefix to add to the pid file which will otherwise be
e.g. 0.pid for the first server in the cluster, or 1.pid for the
second, etc.</li>
<li>addrs: The list of all resolver servers in this level of the
cluster, e.g. not children or parents, just this level. When
starting the server you must pass in an index into this array on the
command line as --id to identify which server you want to start.</li>
<li>max_connections: The maximum number of simultaneous connections to
allow (both read and write) before starting to reject new
connections.</li>
<li>hello_timeout: The amount of time to wait for a client to say a
proper hello before dropping the connection.</li>
<li>reader_ttl: The amount of time, in seconds, to keep an idle read
connection open.</li>
<li>writer_ttl: The amount of time, in seconds, to wait for a publisher
to heartbeat before deleting everything it has published. The
publisher will send heartbeats at 1/2 this interval. e.g. 120 means
publishers will heartbeat every minute.</li>
<li>auth: either &quot;Anonymous&quot;, or &quot;Krb5&quot;. If &quot;Krb5&quot;, then a service
principal name should be included for every resolver server in the
cluster. Each resolver server instance must have access to the
corresponding SPN's key via a keytab or other means, and of course
you must create the corresponding service principal for each
instance.</li>
</ul>
<p>If you're using Kerberos then you also need a permissions file, which
is covered in the next section.</p>
<h1><a class="header" href="#authorization" id="authorization">Authorization</a></h1>
<p>When using Kerberos we also need a permissions file in order to run a
resolver server, it's a separate file because it's not meant to be
shared with everyone using the cluster. e.g.</p>
<pre><code class="language-json">{
    &quot;/&quot;: {
        &quot;eric@RYU-OH.ORG&quot;: &quot;swlpd&quot;
    },
    &quot;/solar&quot;: {
	    &quot;svc_solar@RYU-OH.ORG&quot;: &quot;pd&quot;
    }
}
</code></pre>
<p>In order to do the corresponding action in netidx a user must have
that permission bit set. Permission bits are computed starting from
the root proceeding down the tree to the node being acted on. The bits
are accumulated on the way down. Each bit is represented by a 1
character symbolic tag, e.g.</p>
<ul>
<li>!: Deny, changes the meaning of the following bits to deny the
corresponding permission instead of grant it. May only be the first
character of the permission string.</li>
<li>s: Subscribe</li>
<li>w: Write</li>
<li>l: List</li>
<li>p: Publish</li>
<li>d: Publish default</li>
</ul>
<p>For example if I was subscribing to
<code>/solar/stats/battery_sense_voltage</code> we would walk down the path from
left to right and hit this permission first,</p>
<pre><code class="language-json">&quot;/&quot;: {
    &quot;eric@RYU-OH.ORG&quot;: &quot;swlpd&quot;
},
</code></pre>
<p>This applies to a Kerberos principal &quot;eric@RYU-OH.ORG&quot;, the resolver
server will check the user principal name of the user making the
request, and it will check all the groups that user is a member of,
and if any of those are &quot;eric@RYU-OH.ORG&quot; then it will <code>or</code> the
current permission set with &quot;swlpd&quot;. In this case this gives me
permission to do anything I want in the whole tree (unless it is later
denied). Next we would hit,</p>
<pre><code class="language-json">&quot;/solar&quot;: {
    &quot;svc_solar@RYU-OH.ORG&quot;: &quot;pd&quot;
}
</code></pre>
<p>Which doesn't apply to me, and so would be ignored, and since there
are no more permissions entries my effective permissions at
<code>/solar/stats/battery_sense_voltage</code> are &quot;swlpd&quot;, and so I would be
allowed to subscribe.</p>
<p>Suppose however I changed the above entry,</p>
<pre><code class="language-json">&quot;/solar&quot;: {
    &quot;svc_solar@RYU-OH.ORG&quot;: &quot;pd&quot;,
    &quot;eric@RYU-OH.ORG&quot;: &quot;!swl&quot;,
}
</code></pre>
<p>Now, in our walk, when we arrived at <code>/solar</code>, we would find an entry
that matches me, and we would remove the permission bits s, w, and l,
leaving our effective permissions at
<code>/solar/stats/battery_sense_voltage</code> as &quot;pd&quot;. Since that doesn't give
me the right to subscribe my request would be denied. We could also do
this by group.</p>
<pre><code class="language-json">&quot;/solar&quot;: {
    &quot;svc_solar@RYU-OH.ORG&quot;: &quot;pd&quot;,
    &quot;RYU-OH\domain admins&quot;: &quot;!swl&quot;,
}
</code></pre>
<p>As you would expect, this deny permission will still apply to me
because I am a member of the domain admins group. If I am a member of
two groups, and both groups have different bits denied, then all of
them would be removed. e.g.</p>
<pre><code class="language-json">&quot;/solar&quot;: {
    &quot;svc_solar@RYU-OH.ORG&quot;: &quot;pd&quot;,
    &quot;RYU-OH\domain admins&quot;: &quot;!swl&quot;,
    &quot;RYU-OH\enterprise admins&quot;: &quot;!pd&quot;,
}
</code></pre>
<p>Now my effective permissions under <code>/solar</code> are empty, I can do
nothing. If I am a member of more than one group, and one denies
permissions that the other grants the deny always takes precidence.</p>
<p>Each server cluster is completely independent for permissions. If for
example this cluster had a child cluster, the administrators of that
cluster would be responsible for deciding what permissions file it
should use. It certainly could use the same file, but it doesn't have
to.</p>
<h3><a class="header" href="#anonymous" id="anonymous">Anonymous</a></h3>
<p>It's possible to give anonymous users permissions even on a Kerberos
enabled system, and this could allow them to use whatever functions
you deem non sensitive, subject to some limitations. There is no
encryption. There is no tamper protection. There is no publisher -&gt;
subscriber authentication. Anonymous users can't subscribe to non
anonymous publishers. Non anonymous users can't subscribe to anonymous
publishers. You name anonymous &quot;&quot; in the permissions file, e.g.</p>
<pre><code class="language-json">&quot;/tmp&quot;: {
    &quot;&quot;: &quot;swlpd&quot;
}
</code></pre>
<p>Now <code>/tmp</code> is an anonymous free for all. If you have Kerberos
deployed, it's probably not that useful to build such a hybrid system,
because any anonymous publishers would not be usable by kerberos
enabled users. However it might be useful if you have embedded systems
that can't use kerberos, and you don't want to build a separate
resolver server infrastructure for them.</p>
<h3><a class="header" href="#groups" id="groups">Groups</a></h3>
<p>You'll might have noticed I'm using AD style group names above, that's
because my example setup uses Samba in ADS mode so I can test windows
and unix clients on the same domain. The most important thing about
the fact that I'm using Samba ADS and thus have the group names I have
is that it doesn't matter. Groups are just strings to netidx, for a
given user, whatever the <code>id</code> command would spit out for that user is
what it's going to use for the set of groups the user is in (so that
better match what's in your permissions file). You need to set up the
resolver server machines such that they can properly resolve the set
of groups every user who might use netidx is in.</p>
<p>Luckily you only need to get this right on the machines that run
resolver servers, because that's the only place group resolution
happens in netidx. You're other client and server machines can be as
screwed up and inconsistent as you want, as long as the resolver
server machine agrees that I'm a member of &quot;RYU-OH\domain admins&quot; then
whatever permissions assigned to that group in the permission file
will apply to me.</p>
<p>All the non resolver server machines need to be able to do is get
Kerberos tickets. You don't even need to set them up to use Kerberos
for authentication (but I highly recommend it, unless you really hate
your users), you can just force people to type <code>kinit foo@BAR.COM</code>
every 8 hours if you like.</p>
<h1><a class="header" href="#running-the-resolver-server" id="running-the-resolver-server">Running the Resolver Server</a></h1>
<p>As of this writing the resolver server only runs on Unix, and has only
been extensively tested on Linux. There's no reason it couldn't run on
Windows, it's just a matter of some work around group name resolution
and service integration. Starting a resolver server is done from the
<code>netidx</code> command line tool (<code>cargo install netidx-tools</code>). e.g.</p>
<pre><code class="language-bash">$ KRB5_KTNAME=FILE:/path/to/keytab \
netidx resolver-server --permissions ./netidx-perms.json
</code></pre>
<p>By default the server will daemonize, include <code>-f</code> to prevent that. If
your cluster has multiple replica servers then you must pass <code>--id &lt;index&gt;</code> to specify which one you are starting, however since the
default is 0 you can omit the id argument in the case where you only
have 1 replica.</p>
<p>You can test that it's working by running,</p>
<pre><code class="language-bash">$ netidx resolver list /
</code></pre>
<p>Which should print nothing (since you have nothing published), but
should not error, and should run quickly. You can use the command line
publisher and subscriber to further test. In my case I can do,</p>
<pre><code class="language-bash">[eric@blackbird ~]$ netidx publisher \
    --bind 192.168.0.0/24 \
    --spn host/blackbird.ryu-oh.org@RYU-OH.ORG &lt;&lt;EOF
/test|string|hello world
EOF
</code></pre>
<p>and then I can subscribe using</p>
<pre><code class="language-bash">[eric@blackbird ~]$ netidx subscriber /test
/test|string|hello world
</code></pre>
<p>you'll need to make sure you have permission, that you have a keytab
you can read with that spn in it, and that the service principal
exists etc. You may need to, for example, run the publisher and/or
resolver server with</p>
<p><code>KRB5_KTNAME=FILE:/somewhere/keytabs/live/krb5.keytab</code></p>
<p><code>KRB5_TRACE=/dev/stderr</code> can be useful in debugging kerberos issues.</p>
<h2><a class="header" href="#subscription-flow" id="subscription-flow">Subscription Flow</a></h2>
<p>Sometimes debugging problems requires a more detailed understanding of
exactly what steps are involved in a subscription.</p>
<h3><a class="header" href="#components" id="components">Components</a></h3>
<p><img src="subscription-flow-components.png" alt="The Components" /></p>
<p>In the full kerberos enabled version of netidx the following
components are involved.</p>
<ul>
<li>The Kerberos 5 KDC (Key Distribution Center). e.g. The AD Domain Controller.</li>
<li>Resolver Cluster, holds the path of everything published and the
address of the publisher publishing it.</li>
<li>Subscriber</li>
<li>Publisher, holds the actual data, and has previously told the
resolver server about the path of all the data it has.</li>
</ul>
<h3><a class="header" href="#step-1" id="step-1">Step 1</a></h3>
<p><img src="subscription-flow-step1.png" alt="First Step" /></p>
<ol>
<li>The Subscriber asks the KDC for a service ticket to talk to the
Resolver Cluster. Note this only happens once for each user for
some amount of time (usually hours), after which the service ticket
is cached. The subscriber proves it's identity to the KDC using
it's TGT.</li>
<li>The KDC, having checked the validity of the subscriber's identity,
generates a service ticket for the resolver server cluster. NOTE,
Kerberos does not make authorization decisions, it merely allows
entities to prove to each other that they are who they claim to be.</li>
</ol>
<h3><a class="header" href="#step-2" id="step-2">Step 2</a></h3>
<p><img src="subscription-flow-step2.png" alt="Second Step" /></p>
<ol start="3">
<li>The Subscriber uses the service ticket to establish an encrypted
GSSAPI session with the Resolver Cluster.</li>
<li>Using the session it just established sends a resolve request for
the paths it wants to subscribe to. All traffic is encrypted using
the session.</li>
<li>The Resolver Cluster verifies the presented GSSAPI token and
establishes a secure session, looks up the requested paths, and
returns a number of things to the subscriber for each path.
<ul>
<li>The addresses of all the publishers who are publishing that path</li>
<li>The service principal names of those publishers</li>
<li>The permissions the subscriber has to the path</li>
<li>The authorization token, which is a SHA512 hash of the concatenation of
<ul>
<li>A secret shared by the Resolver Cluster and the Publisher</li>
<li>The path</li>
<li>The permissions</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3><a class="header" href="#step-3" id="step-3">Step 3</a></h3>
<p><img src="subscription-flow-step3.png" alt="Third Step" /></p>
<ol start="6">
<li>The subscriber picks a random publisher from the set of publishers
publishing the path it wants, and requests a service ticket for
that publisher's SPN from the KDC.</li>
<li>The KDC validates the subscriber's TGT and returns a service ticket
for the requested SPN, which will be cached going forward (usually
for several hours).</li>
</ol>
<h3><a class="header" href="#step-4" id="step-4">Step 4</a></h3>
<p><img src="subscription-flow-step4.png" alt="Fourth Step" /></p>
<ol start="8">
<li>
<p>The subscriber uses the service ticket it just obtained to
establish an encrypted GSSAPI session with the publisher, and using
this session it sends a subscribe request, which consists of,</p>
<ul>
<li>The path it wants to subscribe to</li>
<li>The permissions the resolver cluster gave to it</li>
<li>The authorization token</li>
</ul>
</li>
<li>
<p>The publisher validates the subscriber's GSSAPI token and
establishes an encrypted session, and then reads the subscribe
request. It looks up the request path, and assuming it is
publishing that path, it constructs a SHA512 hash value of,</p>
<ul>
<li>The secret it shared with the resolver cluster when it initially
published the path.</li>
<li>The path the subscriber is requesting</li>
<li>The permissions the subscriber claims to have </li>
</ul>
<p>It then checks that it's constructed auth token matches the one the
subscriber presented. Since the subscriber does not know the secret
the publisher shared with the resolver server it is computationally
infeasible for the subscriber to generate a valid hash value for an
arbitrary path or permissions, therefore checking this hash is an
effective proof that the resolver cluster really gave the
subscriber the permissions it is claiming to have.</p>
<p>Assuming all the authentication and authorization checks out, and
the publisher actually publishes the requested value, it sends the
current value back to the publisher along with the ID of the
subscription.</p>
<p>Whenever the value changes the publisher sends the new value along
with the ID of the subscription to the publisher (encrypted using
the GSSAPI session, and over the same TCP session that was
established earlier).</p>
</li>
</ol>
<p>In the case netidx is not configured to use kerberos the KDC is not
involved, and none of the authentication or authorization tokens are
established/sent, it's just a simple matter of look up the address
from the resolver, and then subscribe to the publisher. In that case
all data goes in the clear.</p>
<h1><a class="header" href="#fault-tolerance" id="fault-tolerance">Fault Tolerance</a></h1>
<p>As a system netidx depends on fault tolerant strategies in the
subscriber, publisher, and resolver server in order to minimize
downtime caused by a failure. Before I talk about the specific
strategies used by each component I want to give a short taxonomy of
faults as I think of them so we can be clear about what I'm actually
talking about.</p>
<ul>
<li>Hang: Where a component of the system is not 'dead', e.g. the
process is still running, but is no longer responding, or is so slow
it may as well not be responding. IMHO this is the worst kind of
failure. It can happen at many different layers, e.g.
<ul>
<li>You can simulate a hang by sending SIGSTOP to a unix process. It
isn't dead, but it also won't do anything.</li>
<li>A machine with a broken network card, such that most packets are
rejected due to checksum errors, it's still on the network, but
it's effective bandwidth is a tiny fraction of what it should be.</li>
<li>A software bug causing a deadlock</li>
<li>A malfunctioning IO device</li>
</ul>
</li>
<li>Crash: A process or the machine it's running on crashes cleanly and
completely.</li>
<li>Bug: A semantic bug in the system that causes an effective end to
service.</li>
<li>Misconfiguration: An error in the configuration of the system that
causes it not to work. e.g.
<ul>
<li>Resolver server addresses that are routeable by some clients and not others</li>
<li>Wrong Kerberos SPNs</li>
<li>Misconfigured Kerberos</li>
</ul>
</li>
</ul>
<h3><a class="header" href="#subscriber--publisher" id="subscriber--publisher">Subscriber &amp; Publisher</a></h3>
<ul>
<li>
<p>Hang: Most hang situations are solved by heartbeats. Publisher sends
a heartbeat to every subscriber that is connected to it every 5
seconds. Subscriber disconnects if it doesn't reveive at least 1
message every 100 seconds.</p>
<p>Once a hang is detected it is dealt with by disconnecting, and it
essentially becomes a crash.</p>
<p>The hang case that heartbeats don't solve is when data is flowing,
but not fast enough. This could have multiple causes e.g. the
subscriber is too slow, the publisher is too slow, or the link
between them is too slow. Whatever the cause, the publisher can
handle this condition by providing a timeout to it's <code>flush</code>
function. This will cause any subscriber that can't consume the
flushed batch within the specified timeout to be disconnected.</p>
</li>
<li>
<p>Crash: Subscriber allows the library user to decide how to deal with
a publisher crash. If the lower level <code>subscribe</code> function is used
then on being disconnected unexpecetedly by the publisher all
subscriptions are notified and marked as dead. The library user is
free to retry. The library user could also use <code>durable_subscribe</code>
which will dilligently keep trying to resubscribe, with linear
backoff, until it is successful. Regardless of whether you retry
manually or use <code>durable_subscribe</code> each retry will go through the
entire process again, so it will eventually try all the publishers
publishing a value, and it will pick up any new publishers that
appear in the resolver server.</p>
</li>
</ul>
<h3><a class="header" href="#resolver" id="resolver">Resolver</a></h3>
<ul>
<li>Hang: Resolver clients deal with a resolver server hang with a
dynamically computed timeout based on the number of requests in the
batch. The rule is, minimum timeout 15 seconds or 6 microseconds per
operation in the batch for reads or 12 microseconds per operation in
the batch for writes, whichever is longer. That timeout is a timeout
to get an answer, not to finish the batch. Since the resolver server
breaks large batches up into smaller ones, and answers each micro
batch when it's done, the timeout should not usually be hit if the
resolver is just very busy, since it will be sending back something
periodically for each micro batch. The intent is for the timeout to
trigger if the resolver is really hanging.</li>
<li>Crash: Resolver clients deal with crashes differently depending on
whether they are read or write connections.
<ul>
<li>Read Connections (Subscriber): Abandon the current connection, wait a random
time between 1 and 4 seconds, and then go through the whole
connection process again. That roughly entails taking the list of
all servers, permuting it, and then connecting to each server in
the list until one of them answers, says a proper hello, and
successfully authenticates (if kerberos is on). For each batch a
resolver client will do this abandon and reconnect dance 3 times,
and then it will give up and return an error for that
batch. Subsuquent batches will start over from the beginning. In a
nutshell read clients will,
<ul>
<li>try every server 3 times in a random order</li>
<li>only give up on a batch if every server is down or unable to answer</li>
<li>remain in a good state to try new batches even if previous batches have failed</li>
</ul>
</li>
<li>Write Connections (Publishers): Since write connections are
responsible for replicating their data out to each resolver server
they don't include some of the retry logic used in the read
client. They do try to replicate each batch 3 times seperated by a
1-4 second pause to each server in the cluster. If after 3 tries
they still can't write to one of the servers then it is marked as
degraded. The write client must send heartbeats periodically
(configurable 1/2 writer_ttl), and it will try to replicate to a
degraded server at each heartbeat interval. In a nutshell write clients,
<ul>
<li>try 3 times to write to each server</li>
<li>try failed servers again each 1/2 <code>writer_ttl</code></li>
<li>never fail a batch, just log an error and keep trying next 1/2 <code>writer_ttl</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>One important consequence of the write client behavior is that in the
event all the resolver servers crash, when they come back up
publishers will republishing everything after a maximum of 1/2
<code>writer_ttl</code> has elapsed.</p>
<h1><a class="header" href="#command-line-tools" id="command-line-tools">Command Line Tools</a></h1>
<p>You don't need to program to use netidx, it comes with a set of useful
command line tools!</p>
<h1><a class="header" href="#command-line-publisher" id="command-line-publisher">Command Line Publisher</a></h1>
<p>The command line publisher allows you to publish values to netidx from
stdin. The format of a published value is pipe separated, and newline
delimited. It has 3 fields,</p>
<ul>
<li>The path</li>
<li>The type which is one of,
<ul>
<li><code>u32</code>: unsigned 32 bit integer, 4 bytes on the wire</li>
<li><code>v32</code>: unsigned 32 bit integer <a href="https://en.wikipedia.org/wiki/LEB128">LEB128 encoded</a>, 1-5 bytes on the wire depending on how big the number is. e.g. 0-128 is just 1 byte</li>
<li><code>i32</code>: signed 32 bit integer, 4 bytes on the wire</li>
<li><code>z32</code>: signed 32 bit integer <a href="https://en.wikipedia.org/wiki/LEB128">LEB128 encoded</a> 1-5 bytes on the wire</li>
<li><code>u64</code>: unsigned 64 bit integer, 8 bytes on the wire</li>
<li><code>v64</code>: unsigned 64 bit integer <a href="https://en.wikipedia.org/wiki/LEB128">LEB128 encoded</a>, 1-10 bytes on the wire</li>
<li><code>i64</code>: signed 64 bit integer, 8 bytes on the wire</li>
<li><code>z64</code>: signed 64 bit integer <a href="https://en.wikipedia.org/wiki/LEB128">LEB128 encoded</a>, 1-10 bytes on the wire</li>
<li><code>f32</code>: 32 bit single precision floating point number, 4 bytes on the wire</li>
<li><code>f64</code>: 64 bit double precision floating point number, 8 bytes on the wire</li>
<li><code>datetime</code>: a date + time encoded as an i64 timestamp representing
the number of seconds since jan 1 1970 UTC and a u32 number of sub
second nanoseconds fixing the exact point in time. 12 bytes on the
wire</li>
<li><code>duration</code>: a duration encoded as a u64 number of seconds plus a u32
number of sub second nanoseconds fixing the exact duration. 12 bytes on the wire</li>
<li><code>bool</code>: true, or false. 1 byte on the wire</li>
<li><code>string</code>: a unicode string, limited to 1 GB in length. Consuming 1-10 + number of bytes in the string on the wire (the length is LEB128 encoded)</li>
<li><code>bytes</code>: a byte array, limited to 1 GB in length, Consuming 1-10 + number of bytes in the array on the wire</li>
<li><code>result</code>: OK, or Error + string, consuming 1-1+string length bytes</li>
</ul>
</li>
<li>The value</li>
</ul>
<p>or the special form</p>
<ul>
<li>The path</li>
<li><code>null</code></li>
</ul>
<p>For example <code>/the/path/to/the/thing|u32|42</code>. If you want to publish to
a path that has a <code>|</code> character in it then you must escape the <code>|</code>
with <code>\</code>, e.g. <code>\|</code>. If you want to publish a path that has a <code>\</code> in
it, then you must also escape it, e.g. <code>\\</code>.</p>
<h2><a class="header" href="#arguments" id="arguments">Arguments</a></h2>
<p>There are several command line options to the <code>netidx publisher</code> command,</p>
<ul>
<li><code>--bind</code>: required, specify the network address to bind to. This can
be specified in two forms.
<ul>
<li>an exact address and port e.g.
<ul>
<li>127.0.0.1:5000</li>
<li>127.0.0.1:0, in which case the OS will choose the port at random</li>
</ul>
</li>
<li>an expression consisting of an ip/netmask that must match a unique
network interface on the machine running the publisher. e.g.
<ul>
<li>127.0.0.0/24 selects the <code>lo</code> interface</li>
<li>10.0.0.0/8 selects the interface bound to a 10.x.x.x address</li>
<li>192.168.0.0/16 selects the interface bound to a 192.168.x.x address</li>
</ul>
</li>
</ul>
</li>
<li><code>--spn</code>: optional, required if using krb5, the service principal
name the publisher should run as. This principal must have
permission to publish where you plan to publish, must exist in your
krb5 infrastructure, and you must have access to a keytab with it's
credentials. If that keytab is in a non standard location then you
must set the environment variable
<code>KRB5_KTNAME=FILE:/the/path/to/the/keytab</code></li>
<li><code>--timeout &lt;seconds&gt;</code>: optional, if specified requires subscribers
to consume published values within the specified number of seconds
or be disconnected. By default the publisher will wait forever for a
subscriber to consume an update, and as a result could consume an
unbounded amount of memory.</li>
</ul>
<h2><a class="header" href="#behavior" id="behavior">Behavior</a></h2>
<p>When started the publisher runs until killed, it reads lines from
stdin as long as stdin remains open, and attempts to parse them as
<code>PATH|TYPE|VALUE</code> triples. If parsing fails, it prints an error to
stderr and continues reading. If parsing succeeds it checks if it has
already published <code>PATH</code>, if not, it publishes it with the specified
type and value, if it has, then it updates the existing published
value. It is not an error to change the type of an existing published
value. If stdin is closed publisher does not stop, however it is no
longer possible to update existing published values, or publish new
values without restarting it.</p>
<h2><a class="header" href="#limitations" id="limitations">Limitations</a></h2>
<p>The command line publisher cannot respond to write requests, and
cannot be a default publisher.</p>
<h2><a class="header" href="#environment-variables" id="environment-variables">Environment Variables</a></h2>
<p>In addition to all the krb5 environment variables, the command line
publisher uses envlogger, and so will respond to <code>RUST_LOG</code>,
e.g. <code>RUST_LOG=debug</code> will cause the publisher to print debug and
higher priority messages to stderr.</p>
<h1><a class="header" href="#command-line-subscriber" id="command-line-subscriber">Command Line Subscriber</a></h1>
<p>The command line subscriber allows you to subscribe to values in
netidx. You can either specify a list of paths you want to subscribe
to on the command line, or via commands sent to stdin. Once subscribed
a line in the form <code>PATH|TYPE|VALUE</code> will be printed for every update
to a subscribed value, including the initial value. e.g. on my local
network I can get the battery voltage of my solar array by typing,</p>
<pre><code>netidx subscriber /solar/stats/battery_sense_voltage
/solar/stats/battery_sense_voltage|f32|26.796875
</code></pre>
<h2><a class="header" href="#directives-via-stdin" id="directives-via-stdin">Directives via stdin</a></h2>
<p>The command line subscriber reads commands from stdin which can direct it to,</p>
<ul>
<li>subscribe to a new path
<ul>
<li><code>ADD|/path/to/thing/you/want/to/add</code></li>
</ul>
</li>
<li>end a subscription
<ul>
<li><code>DROP|/path/to/thing/you/want/to/drop</code></li>
</ul>
</li>
<li>write a value to a subscribed path
<ul>
<li><code>WRITE|/path/to/thing/you/want/to/write|TYPE|VALUE</code></li>
<li>if the path you are writing to has a <code>|</code> in it, then you must
escape it, e.g. <code>\|</code>. If it has a literal <code>\</code> in it, then you also
must escape it e.g. <code>\\</code>.</li>
</ul>
</li>
</ul>
<p>If the subscriber doesn't recognize a command it will print an error
to stderr and continue reading commands. If stdin is closed subscriber
will not quit, but it will no longer be possible to issue commands.</p>
<h2><a class="header" href="#notes" id="notes">Notes</a></h2>
<p>The format subscriber writes to stdout is compatible with the format
the publisher reads. This is by design, to make applications that
subscribe, manipulate, and republish data easy to write.</p>
<h1><a class="header" href="#resolver-command-line-tool" id="resolver-command-line-tool">Resolver Command Line Tool</a></h1>
<p>The resolver command line tool allows you to query and update the
resolver server from the command line. It's mostly intended for
testing and debugging. There are several kinds of querys/manipulations it can perform,</p>
<ul>
<li><code>list</code>: list entries matching a specified pattern</li>
<li><code>table</code>: query the table descriptor for a path</li>
<li><code>resolve</code>: see what a path resolves to</li>
<li><code>add</code>: add a new entry</li>
<li><code>remove</code>: remove an entry</li>
</ul>
<h2><a class="header" href="#list" id="list">List</a></h2>
<p><code>netidx resolver list /solar/stats/*</code></p>
<p>This sub command allows listing items in the resolver server that
match a specific pattern. It supports the full unix glob pattern set,
including <code>**</code> meaning any number of intermediate parents, and
<code>{thing1, thing2, thing3, ..}</code> for specifying specific sets. e.g. on
my machine I can get all the names under the <code>/solar</code> namespace that
begin with <code>battery</code> with the following query,</p>
<pre><code>$ netidx resolver list /solar/**/battery*
/solar/settings/battery_charge_current_limit
/solar/stats/battery_v_max_daily
/solar/stats/battery_current_net
/solar/stats/battery_voltage_slow
/solar/stats/battery_voltage_settings_multiplier
/solar/stats/battery_sense_voltage
/solar/stats/battery_v_min_daily
/solar/stats/battery_temperature
/solar/stats/battery_terminal_voltage
</code></pre>
<h3><a class="header" href="#args" id="args">Args</a></h3>
<p>List supports several arguments,</p>
<ul>
<li><code>-n, --no-structure</code>: don't list matching items that are structural
only. Only list items that are actually published.</li>
<li><code>-w, --watch</code>: don't quit, instead poll the resolver server for
changes to things matching the specified pattern, and print out new
items as they are added. This does not just run the query every
second, since the resolver server maintains a set of change ids for
each path, using those ids this command is able to run the query
again only if something relevant has actually changed.</li>
</ul>
<h2><a class="header" href="#table" id="table">Table</a></h2>
<p>This prints out the table desciptor for a path, which can tell you how
a given path will look in the browser by default. An example 10 row 5
column table generated by the stress publisher looks like this,</p>
<pre><code>$ netidx resolver table /bench
columns:
2: 10
0: 10
1: 10
4: 10
3: 10
rows:
/bench/3
/bench/5
/bench/6
/bench/9
/bench/8
/bench/7
/bench/2
/bench/4
/bench/1
/bench/0
</code></pre>
<p>in the columns section, the number after the column name is the number
of rows in the table that have that column. Since this table is fully
populated every column is associated with 10 rows.</p>
<h2><a class="header" href="#resolve" id="resolve">Resolve</a></h2>
<p>Given a path this prints out all the information the resolver server
has about that path. This is what the subscriber uses to connect to a
publisher, and as such this tool is useful for debugging subscription
failures. Using the same stress publisher we saw above we can query
one cell in the table.</p>
<pre><code>$ netidx resolver resolve /bench/0/0
resolver: 192.168.0.1:4564
192.168.0.5:5004: publish/blackbird.ryu-oh.org@RYU-OH.ORG
192.168.0.5:5004
</code></pre>
<p>First the address of the resolver that answered the query is printed,
then a list of service principal names corresponding to each address,
and then all the addresses for all the publishers publishing the
requested path. In this case there is one publisher with one service
principal name.</p>
<p>In the case that nothing is published for a given path, then just the
address of the resolver that answered will be printed. e.g.</p>
<pre><code>$ netidx resolver resolve /bench
resolver: 192.168.0.1:4564
</code></pre>
<h2><a class="header" href="#add" id="add">Add</a></h2>
<p>This is a low level debugging tool, and it's really not recommended
unless you know exactly what you're doing. Using it could screw up
subscriptions to whatever path you add for some time. That said, it's
pretty simple,</p>
<pre><code>netidx resolver add /path/to/thing 192.168.0.5:5003
</code></pre>
<p>This entry will time out after a while because no publisher is there
to send heartbeats for it.</p>
<p>Note this will not work if your system is kerberos enabled, because
the resolver server checks that the publisher is actually listening on
the address it claims to be listening on, and that obviously can't
work in this case.</p>
<h2><a class="header" href="#remove" id="remove">Remove</a></h2>
<p>This is actually worse than add in terms of danger, because you can
remove published things without the publisher knowing you did it, and
as a result you can make subscriptions fail until the publisher is
restarted. It also doesn't work if you are using kerberos, so that's something.</p>
<pre><code>netidx resolver remove /path/to/thing 192.168.0.5:5003`
</code></pre>
<h1><a class="header" href="#recorder" id="recorder">Recorder</a></h1>
<p>The recorder allows you to subscribe to a set of paths defined by one
or more globs and write down their values in a file with a compact
binary format. Moreover, at the same time it can make the contents of
an archive available for playback by multiple simultaneous client
sessions, each with a potentially different start time, playback
speed, end time, and position.</p>
<p>It's possible to set up a recorder to both record data and play it
back at the same time, or only record, or only play back. It is not
possible to set up one recorder to record, and another to play back
the same file, however recording and playback are careful not to
interfere with each other, so the only limitation should be the
underlying IO device and the number of processor cores available.</p>
<h2><a class="header" href="#args-1" id="args-1">Args</a></h2>
<ul>
<li>Args that apply to both recording and playback
<ul>
<li><code>--archive &lt;file&gt;</code>: required by both modes of operation, the name
of the file to record the data into or play it back from. If an
existing archive is specified and recording is requested then the
data will be appended to that archive.</li>
<li><code>--shards &lt;n&gt;</code>: optional, The number of recorder shards to
expect. If you want to record/playback a huge namespace, or one
that updates a lot, it may not be possible to use just one
computer. The recorder supports sharding across an arbitrary
number of processes for both recording and playback. n is the
number of shards that are expected in a given
cluster. recording/playback will not begin until all the shards
have appeared and synced with each other. default 1.</li>
<li><code>-f, --foreground</code>: don't daemonize</li>
</ul>
</li>
<li>Args that apply to recording
<ul>
<li><code>--spec &lt;glob&gt;</code>: required by recording, enables recording if
specified, may be specified multiple times, a glob describing what
to archive. If multiple globs are specified and they overlap, the
overlapped items will only be archived once.</li>
<li><code>--flush-frequency &lt;pages&gt;</code>: optional, How much data to write before
flushing to disk in pages, where a page is a filesystem page,
however large that is on your system. default 65534. This is the
maximum amount of data you will probably lose in a power outage,
system crash, or program crash. The recorder uses two phase commits
to the archive file to ensure that partially written data does not
corrupt the file.</li>
<li><code>--flush-interval &lt;seconds&gt;</code>: optional, How long in seconds to wait
before flushing data to disk even if <code>flush-frequency</code> pages was not
yet written. 0 to disable, default 30.</li>
<li><code>--image-frequency &lt;bytes&gt;</code>: optional, How often, in bytes, to write a full
image of every current value, even if it did not update. Writing
images increases the file size, but makes seeking to an arbitrary
position in the archive much faster. 0 to disable images, in which
case a seek back will read all the data before the requested
position, default 64MiB.</li>
<li><code>--poll-interval &lt;seconds&gt;</code>: optional, How often, in seconds, to
poll the resolver server for changes to the specified glob set. 0
never poll, default 5.</li>
</ul>
</li>
<li>Args that apply to playback
<ul>
<li><code>--publish-base &lt;path&gt;</code>: required for playback, enables playback
if specified, the path where playback sessions will be published.</li>
<li><code>--bind &lt;spec&gt;</code>: required for playback, a specification describing
the network interface to bind to. See
<a href="./publisher_tool.html">publisher</a> for details.</li>
<li><code>--spn &lt;service-principal&gt;</code>: optional, required for kerberos, the
service princial to publish as.</li>
<li><code>--max-sessions &lt;n&gt;</code>: optional, How many total client sessions to allow at
any given time. When a session is no longer used, it will be
garbage collected. default 256.</li>
<li><code>--max-sessions-per-client &lt;n&gt;</code>: optional, The maximum number of
sessions a single client is allowed to have. default 64.</li>
</ul>
</li>
</ul>
<h2><a class="header" href="#using-playback-sessions" id="using-playback-sessions">Using Playback Sessions</a></h2>
<p>When initially started for playback or mixed operation the recorder
publishes only some cluster information, and a netidx rpc called
<code>session</code> under the <code>publish-base</code>. Calling the session rpc will
create a new session, and return the session id. Then it will publish
the actual playback session under <code>publish-base/session-id</code>. A
playback session consists of two sub directories, <code>control</code> contains
readable/writable values that control the session, and <code>data</code> contains
the actual data.</p>
<h3><a class="header" href="#creating-a-new-session" id="creating-a-new-session">Creating a New Session</a></h3>
<p>It's simple to call a netidx rpc with command line tools, the browser,
or programatically. To create a new playback session with default
values just write <code>null</code> to <code>publish-base/session</code>. e.g.</p>
<pre><code>netidx subscriber /archive/session
/solar/archive/session|none|Null
WRITE|/solar/archive/session|string|null
/solar/archive/session|string|ef93a9dce21f40c49f5888e64964f93f
</code></pre>
<p>We just created a new playback session called
ef93a9dce21f40c49f5888e64964f93f, we can see that the recorder
published some new things there,</p>
<pre><code>$ netidx resolver list /solar/archive/ef93a9dce21f40c49f5888e64964f93f/*
/solar/archive/ef93a9dce21f40c49f5888e64964f93f/data
/solar/archive/ef93a9dce21f40c49f5888e64964f93f/cluster
/solar/archive/ef93a9dce21f40c49f5888e64964f93f/control
</code></pre>
<p>If we want to pass some arguments to the rpc so our session will be
setup how we like by default we can do that as well, e.g.</p>
<pre><code>netidx subscriber \
    /solar/archive/session \
    /solar/archive/session/start/val \
    /solar/archive/session/speed/val
/solar/archive/session|none|Null
/solar/archive/session/start/val|string|Unbounded
/solar/archive/session/speed/val|f64|1
WRITE|/solar/archive/session/start/val|string|-3d
WRITE|/solar/archive/session/speed/val|f32|2
WRITE|/solar/archive/session|string|null
/archive/session|string|ef93a9dce21f40c49f5888e64964f93f
</code></pre>
<p>First we are told the defaults, as a result of subscribing to the
rpc's arguments, then we write our desired values and finally call the
rpc. Now our new session would be setup to start 3 days ago, and
playback at 2x speed.</p>
<h3><a class="header" href="#playback-controls" id="playback-controls">Playback Controls</a></h3>
<p>Once we've created a new session the recorder publishes some controls
under the control directory. The five controls both tell you the state
of the playback session, and allow you to control it. They are,</p>
<ul>
<li><code>start</code>: The timestamp you want playback to start at, or Unbounded
for the beginning of the archive. This will always display
Unbounded, or a timestamp, but it in addition to those two values it
accepts writes in the form of offsets from the current time,
e.g. -3d would set the start to 3 days ago. It accepts offsets
[+-]N[yMdhms] where N is a number. y - years, M - months, d - days,
h - hours, m - minutes, s - seconds.</li>
<li><code>end</code>: Just like start except that Unbounded, or a time in the
future means that when playback reaches the end of the archive it
will switch mode to tail. In tail mode it will just repeat data as
it comes in. In the case that end is in the future, but not
unbounded, it will stop when the future time is reached.</li>
<li><code>pos</code>: The current position, always displayed as a timestamp unless
there is no data in the archive. Pos accepts writes in the form of
timestamps, offsets from the current time (like start and end), and
[+-]1-128 batches. e.g. -10 would seek back exactly 10 update
batches, +100 would seek forward exactly 100 update batches.</li>
<li><code>speed</code>: The playback speed as a fraction of real time, or
Unlimited. In the case of Unlimited the archive is played as fast as
it can be read, encoded, and sent. Otherwise the recorder tries to
play back the archive at aproximately the specified fraction of real
time. This will not be perfect, because timing things on computers
is hard, but it tries to come close.</li>
<li><code>state</code>: this is either play, pause or tail, and it accepts writes
of any state and will change to the requested state if possible.</li>
</ul>
<p>Since the controls also include a small amount of documentation meant
to render as a table, the actual value that you read from/write to is
<code>publish-base/session-id/control/name-of-control/current</code>.</p>
<h3><a class="header" href="#data" id="data">Data</a></h3>
<p>Once the session is set up the data, whatever it may be, appears under
<code>publish-base/data</code>. Every path that ever appears in the archive is
published from the beginning, however, if at the current <code>pos</code> that
path didn't have a value, then it will be set to <code>null</code>. This is a
slightly unfortunate compromise, as it's not possible to tell the
difference between a path that wasn't available, and one that was
intentionally set to null. When you start the playback values will be
updated as they were recorded, including replicating the observed
batching.</p>
<h3><a class="header" href="#deleting-a-playback-session" id="deleting-a-playback-session">Deleting a Playback Session</a></h3>
<p>Simply stop subscribing to any value or control in the session and the
recorder will delete it within about 30 seconds. At the moment there
is no other way to delete a session, but that's a feature that would
be easy to add if it was needed.</p>
<h2><a class="header" href="#example" id="example">Example</a></h2>
<p>To record and publish the archive of the data generated by my solar
installation I use the following command.</p>
<pre><code>netidx record \
    --archive ~/solar \
    --spec '/solar/{control,stats,settings}/**' \
    --bind 192.168.0.0/24 \
    --spn publish/blackbird.ryu-oh.org@RYU-OH.ORG \
    --publish-base /solar/archive
</code></pre>
<h1><a class="header" href="#stress-test-tool" id="stress-test-tool">Stress Test Tool</a></h1>
<p>The stress test tool is mostly for debug and development of netidx
itself. The stress publisher publishes a configurably sized table, and
updates each value after a configurable timeout. The stress subscriber
subscribes to every value in the table published by the stress
publisher, and prints throughput statistics to stdout.</p>
<h1><a class="header" href="#browser" id="browser">Browser</a></h1>
<p>The browser is a graphical tool that visualizes the contents of
netidx, it allows users to navigate to places of interest, view data
in tabular or vector form, and construct applications with custom
views and BScript.</p>
<p>When the browser visits it displays either a vector or a table. A
vector looks like e.g.</p>
<p><img src="./browser-vector-mode.png" alt="Vector mode" /></p>
<p>Where as a table looks like, e.g.</p>
<p><img src="./browser-components-tour.png" alt="Table mode" /></p>
<p>The various components highlighted in red are,</p>
<ol>
<li>The path of the currently visualized subtree. To navigate, you can
click on any of the individual components, e.g. click the <code>/</code> to
navigate to the root, and you can refresh the current view by
clicking the link to the current path, <code>bench</code> in this case. If
you're viewing a custom view stored in a file, then this will be
indicated by the current path changing to
e.g. <code>file://somwhere/in/the/file/system</code>.</li>
<li>The gutter displays the path of the currently selected cell, or, in
a custom view it displays the BScript bound to the currently
selected or hovered object.</li>
<li>Design mode button, click to enter design mode. This will display
the design of the current view, and allow you to edit it. This will
be covered in detail in the next section.</li>
<li>The currently selected cell in a table is highlighed. If you want
to write to the currently selected cell, you can press Ctrl+w, and
a write dialog will appear.</li>
<li>The menu button.</li>
</ol>
<p>You can enter a subtree by double clicking anywhere in a row. To go
up, use the path links as described in 1. You can also go directly to
a specific path by using <code>go</code> from the menu.</p>
<p>A subtree will be visualized as a table if it contains a regular
structure of leaf nodes that are named the same thing for multiple
parents, e.g.</p>
<pre><code>/base/row0/col0
/base/row0/col1
...
/base/rowN/col0
/base/rowN/col1
</code></pre>
<p>This will automatically visualize as a table with N rows named <code>row0 .. rowN</code> and 2 columns named <code>col0</code> and <code>col1</code>. If the regular
structure necessary to infer a table doesn't exist, then the subtree
will be visualized as a vector.</p>
<h1><a class="header" href="#constructing-custom-views" id="constructing-custom-views">Constructing Custom Views</a></h1>
<p>Custom browser views are meant to fill the gap between full blown gui
apps that use netidx, and just tables. The goal is that it should be
possible to create moderately complex applications using custom
browser views and bscript (see <a href="./complete_system.html">Clean Slate System
Design</a> for an example). The first step is to
enter design mode.</p>
<p><img src="./browser-design-mode-empty.png" alt="Design Mode" /></p>
<p>Design mode has 3 parts,</p>
<ol>
<li>The widget tree. As the browser is based on Gtk+, this tree pretty
much directly visualizes the Gtk+ widget tree. In Gtk+ layout is
automatic, and is based on the structure of the widget tree, as
such, you get the visual look you want for your app by manipulating
this tree. You can drag and drop nodes around, and using the
buttons just above the tree you can,
<ul>
<li>create a sibling of the currently selected node</li>
<li>create a child of the currently selected node</li>
<li>delete the currently selected node as well as all of it's children</li>
<li>duplicate the currently selected node as well as all of it's children</li>
<li>undo</li>
</ul>
</li>
<li>This box displays the properties and controls of the currently
selected widget, if any.</li>
<li>The content area; this displays the actual live gui as it is being
built. The content area behaves exactly the same in design mode vs
normal mode, and is fully interactive. It's entirely what you see
is what you will get.</li>
</ol>
<p>The normal default operation of the gui is in fact technically a
custom view, it is however automatically generated for each subtree if
no specific custom view is found for that subtree. The default
generated custom view is a table with the <code>Path</code> property set to the
path of the current subtree. For example, we can click on the table in
the widget tree to see the properties sheet.</p>
<p><img src="./browser-table-properties.png" alt="Table Properties" /></p>
<p>There are several things about this worth pointing out.</p>
<ul>
<li>The widget selected in the widget tree will always be outlined in
blue in the content area.</li>
<li>The table widget has some configurable options that allow us to
control column order, default sort, and which columns even appear.</li>
<li>The layout properties expander is present on nearly every widget,
and allows setting properties that all Gtk+ widets share.</li>
</ul>
<h2><a class="header" href="#our-first-custom-view" id="our-first-custom-view">Our First Custom View</a></h2>
<p>Now, lets make a custom view that displays 2 tables looking at
different subtrees at the same time. To do this, we need a container
widget to hold both tables, in Gtk+ we use a Box widget for this kind
of job. Since layout is procedural, we won't need to fiddle with
coordinates or anything like that, so it's very much like html, we
make a box, put some widgets into it, and let the toolkit figure out
how to make it look nice.</p>
<p><img src="./browser-two-tables-design.png" alt="Two Tables" /></p>
<p>If you've used tools like Glade then this will be quite familiar to
you, except that the content are is completely live while the layout
is being designed. You can save the view you've designed by pressing
the disk icon in the header bar. This will allow you to save the view
definition in a file, or you can write it to a location in netidx.</p>
<p>If you write your custom view to a location in netidx that ends in
<code>.view</code>, e.g. <code>/solar/.view</code>, then when the browser is asked to
navigate to that subtree it will draw your view instead of the default
table.</p>
<h2><a class="header" href="#toggle-buttons" id="toggle-buttons">Toggle Buttons</a></h2>
<p>Lets refine this little solar gui a bit, the top table takes up too
much space, and could be easier to interact with. There is a nice Gtk+
widget called a toggle that is great for both visualizing and
controlling boolean values.</p>
<p><img src="./browser-buttons-and-table.png" alt="Buttons and Table" /></p>
<p>We'd like the toggles and their labels to line up in nice neat columns
and rows, we could accomplish that with boxes, however Gtk+ has a grid
container widget that makes it much easier. We can also see our first
BScript looking at the properties of the first toggle button.</p>
<ul>
<li>enabled: This could be any expression, but in our case it's just the
constant <code>true</code>, as this button is always enabled.</li>
<li>value: <code>load(&quot;/solar/control/charging&quot;)</code> defines the value of the
toggle button, it should evaluate to true or false. In this case we
are loading the value directly from netidx. If the value in netidx
changes, then the toggle button's state will change too.</li>
<li>on change: <code>store(&quot;/solar/control/charging&quot;, event())</code> defines what
happens when the user changes the state of the button. In this case
we store the requested state of the button, which is <code>event()</code>, to
the same path we loaded the value from. It's up to the publisher to
decide whether the new requested state is actually valid right
now. If it is, then it will update it's published value and the
state of the toggle will reflect that.</li>
</ul>
<p>We can test out if these buttons work without leaving design mode. We
can also use the expression inspector, by clicking the tool button
right next to the expression, to construct and debug more complex
expressions. Using that tool will be covered more in the next section
on BScript.</p>
<h1><a class="header" href="#language-overview" id="language-overview">Language Overview</a></h1>
<p>The browser scripting language exists to glue the widgets in your gui
to the inputs and outputs in netidx. If you're familiar with the model
view controller gui paradigm, browser script is the controller
layer. Because of it's purpose it works rather differently from most
scripting languages, in that it is a 'reactive' or 'incremental'
language. Instead of describing e.g. &quot;steps the computer performs when
I click the button&quot; like most other languages browser script describes
the plumbing that events flow through on their way to their final
destination.</p>
<p>For example the event handler for a button might look like so,</p>
<pre><code>store(&quot;[base]/app/do_action&quot;, event())
</code></pre>
<p>The store function writes it's 2nd argument to the netidx path
specified by it's first argument. The event function produces a widget
specific event, in this case it produces a Null whenever the button is
clicked. The path argument is actually a string with an expression
interpolation that will generate the path we will write to. So we can
think of this expression as building an event pipeline that looks
something like this,</p>
<pre><code>load_var(base) ------------1-&gt; concat_string ----
                                ^                |
                                |                |
&quot;/app/do_action&quot; -------------2-                 1
                                                 v
event() -----------------------------------2-&gt; store
</code></pre>
<p>So not only do we write a Null whenever the button is clicked, but we
also change were we write whenever the variable <code>base</code>
changes. Constants like <code>&quot;/app/do_action&quot;</code> never change.</p>
<h1><a class="header" href="#types-and-constants" id="types-and-constants">Types and Constants</a></h1>
<p>Types in browser script correspond to the netidx value type, and are
named,</p>
<ul>
<li>u32: unsigned 4 byte integer</li>
<li>v32: unsigned leb128 encoded integer</li>
<li>i32: signed 4 byte integer</li>
<li>z32: signed leb128 zig-zag encoded integer</li>
<li>u64: unsigned 8 byte integer</li>
<li>v64: unsigned leb128 encoded integer</li>
<li>i64: signed 8 byte integer</li>
<li>z64: signed leb128 zig-zag encoded integer</li>
<li>f32: single precision floating point number</li>
<li>f64: double precision floating point number</li>
<li>bool: boolean</li>
<li>string: unicode string</li>
<li>bytes: byte array</li>
<li>result: ok or error:description of error</li>
</ul>
<p>Constants may be prefixed with the type name followed by a colon, e.g.</p>
<p><code>f32:3.14</code></p>
<p>However constant expressions have a default type if none is specified,</p>
<ul>
<li>floating point numbers: f64</li>
<li>integers: u64</li>
<li>strings: string</li>
<li>true/false: bool</li>
<li>ok: result</li>
</ul>
<p>e.g. <code>3.1415</code> is the same as <code>f64:3.1415</code>, and both forms will be
accepted.</p>
<h1><a class="header" href="#expression-interpolation" id="expression-interpolation">Expression Interpolation</a></h1>
<p>In a string literal you may substitute any number of expressions by
surrounding them with <code>[]</code>. To produce a literal <code>[</code> or <code>]</code> you must
escape them with <code>\</code>, e.g. <code>\[</code> and <code>\]</code>. To produce a literal <code>\</code> you
may escape it as well, e.g. <code>\\</code>. Any expression is a valid
interpolation (including another interpolation), e.g. numeric
expressions will be cast to strings. Any expression that cannot be
cast to a string will be ignored.</p>
<p>e.g.</p>
<pre><code>&quot;[base]/some/path&quot;
&quot;[base]/bar/[if(load(&quot;[base]/enabled&quot;),&quot;enabled&quot;,&quot;disabled&quot;)]/thing&quot;
</code></pre>
<h1><a class="header" href="#the-expression-inspector" id="the-expression-inspector">The Expression Inspector</a></h1>
<p>The expression inspector shows a tree representation of a BScript
expression in it's first column, and the current value of the
corresponding expression in it's second.</p>
<p><img src="./expression-inspector.png" alt="Expression Inspector" /></p>
<p>In this case the expression being inspected is,</p>
<pre><code>store_var(&quot;base&quot;, &quot;[archive_base]/[sessionid]/data&quot;)
</code></pre>
<p>You can access the expression inspector by pressing the toggle button
to the right of the expression box. The inspector can also be used to
edit the expression, updated expressions can be generated by
manipulating the tree.</p>
<h1><a class="header" href="#reference" id="reference">Reference</a></h1>
<h2><a class="header" href="#load" id="load">load</a></h2>
<pre><code>load(Expr)
</code></pre>
<p>Subscribes to the netidx path specified by it's argument, which must
evaluate to a string.</p>
<p>e.g.</p>
<pre><code>load(&quot;/some/path/in/netidx&quot;)
load(&quot;[base]/thing&quot;)
</code></pre>
<h2><a class="header" href="#any" id="any">any</a></h2>
<pre><code>any(Expr, ..., Expr)
</code></pre>
<p>Any produces an event every time any of it's arguments produce an event.</p>
<pre><code>any(42, load(&quot;/foo/bar&quot;), load(&quot;/foo/baz&quot;))
</code></pre>
<p>Will produce 42, and then all the updates to <code>/foo/bar</code> and <code>/foo/baz</code>
in whatever order they arrive.</p>
<pre><code>mean(any(load(&quot;/bench/0/0&quot;), load(&quot;/bench/0/1&quot;)))

</code></pre>
<p>Will produce the average of the values of <code>/bench/0/0</code> and
<code>/bench/0/1</code>.</p>
<h2><a class="header" href="#all" id="all">all</a></h2>
<pre><code>all(Expr, ..., Expr)
</code></pre>
<p>All produces an if the current values of all it's arguments are equal.</p>
<pre><code>all(11, load(&quot;/volume&quot;))
</code></pre>
<p>Will produce 11 only when <code>/volume</code> is 11.</p>
<h2><a class="header" href="#sum" id="sum">sum</a></h2>
<pre><code>sum(Expr, ..., Expr)
</code></pre>
<p>Produces the sum of it's arguments.</p>
<p>e.g.</p>
<pre><code>sum(load(&quot;/offset&quot;), load(&quot;/random&quot;))
</code></pre>
<p>sums <code>/offset</code> and <code>/random</code></p>
<h2><a class="header" href="#product" id="product">product</a></h2>
<pre><code>product(Expr, ..., Expr)
</code></pre>
<p>Produces the product of it's arguments.</p>
<p>e.g. </p>
<pre><code>product(2, 2)
</code></pre>
<h2><a class="header" href="#divide" id="divide">divide</a></h2>
<pre><code>divide(Expr, Expr, ..., Expr)
</code></pre>
<p>Divides it's first argument by it's subsuquent arguments.</p>
<pre><code>divide(load(&quot;/volume&quot;), 2, load(&quot;/additional_divisor&quot;))
</code></pre>
<p>First divides <code>&quot;/volume&quot;</code> by 2 and then divides it by
&quot;/additional_divisor&quot;.</p>
<h2><a class="header" href="#mean" id="mean">mean</a></h2>
<pre><code>mean(Expr)
</code></pre>
<p>Computes the average of it's argument over time.</p>
<p>e.g.</p>
<pre><code>mean(load(&quot;/volume&quot;))
</code></pre>
<p>Produce the average volume over the observed time period.</p>
<h2><a class="header" href="#min" id="min">min</a></h2>
<pre><code>min(Expr, ..., Expr)
</code></pre>
<p>Produces the smallest value of any of it's arguments.</p>
<p>e.g.</p>
<pre><code>min(42, load(&quot;/volume&quot;))
</code></pre>
<p>produces the value of <code>&quot;/volume&quot;</code> if it is less than 42, otherwise it
produces 42.</p>
<h2><a class="header" href="#max" id="max">max</a></h2>
<pre><code>max(Expr, ..., Expr)
</code></pre>
<p>Produces the largest value of any of it's arguments.</p>
<p>e.g.</p>
<pre><code>max(5, load(&quot;/volume&quot;))
</code></pre>
<p>produces the value of &quot;/volume&quot; if it is greater than 5, otherwise it
produces 5.</p>
<h2><a class="header" href="#and" id="and">and</a></h2>
<pre><code>and(Expr, ..., Expr)
</code></pre>
<p>Produces true if all of it's arguments are true, otherwise false.</p>
<p>e.g.</p>
<pre><code>and(load(&quot;/cake&quot;), load(&quot;/diet&quot;))
</code></pre>
<p>Would produce false.</p>
<h2><a class="header" href="#or" id="or">or</a></h2>
<pre><code>or(Expr, ..., Expr)
</code></pre>
<p>Produces true if any of it's arguments is true, otherwise false.</p>
<p>e.g.</p>
<pre><code>or(load(&quot;/cake&quot;), load(&quot;/death&quot;))
</code></pre>
<p>Would produce true.</p>
<h2><a class="header" href="#not" id="not">not</a></h2>
<pre><code>not(Expr)
</code></pre>
<p>Produces the opposite of it's argument, e.g. true if it's argument is
false, false otherwise.</p>
<p>e.g.</p>
<pre><code>not(load(&quot;/solar/control/charging&quot;))
</code></pre>
<p>true if the battery is not charging.</p>
<h2><a class="header" href="#cmp" id="cmp">cmp</a></h2>
<pre><code>cmp(Expr, Expr, Expr)
</code></pre>
<p>Produces the result of performing the comparison specified by it's
first argument to it's 2nd and third arugments. Valid comparisons are
encoded as strings, and are called,</p>
<ul>
<li>eq: true if the arguments are equal</li>
<li>lt: true if the first argument is less than the second one</li>
<li>lte: true if the first argument is less than or equal to the second one</li>
<li>gt: true if the first argument is greater than the second one</li>
<li>gte: true if the first argument is greater than or equal to the second one</li>
</ul>
<p>e.g.</p>
<pre><code>cmp(&quot;lt&quot;, load(&quot;/volume&quot;), 11)
</code></pre>
<p>is true if the volume is less than 11, false otherwise.</p>
<h2><a class="header" href="#if" id="if">if</a></h2>
<pre><code>if(Expr, Expr, [Expr])
</code></pre>
<p>Produces the value of it's 2nd argument if it's first argument is
true, otherwise produces the value of it's third argument, or nothing
if it has no third argument.</p>
<p>e.g.</p>
<pre><code>if(
    cmp(&quot;lt&quot;, load(&quot;/volume&quot;), 11),
    load(&quot;/normal_amp&quot;),
    load(&quot;/this_one_goes_to_11&quot;)
)
</code></pre>
<p>If &quot;/volume&quot; is less than 11 then the value is <code>&quot;/normal_amp&quot;</code>,
otherwise the value is <code>&quot;/this_one_goes_to_11&quot;</code>.</p>
<p>e.g.</p>
<pre><code>if(cmp(&quot;eq&quot;, 11, load(&quot;/volume&quot;)), &quot;huzzah!&quot;)
</code></pre>
<p>Produces <code>&quot;huzzah!&quot;</code> if <code>/volume</code> is <code>11</code>, otherwise nothing.</p>
<h2><a class="header" href="#cast" id="cast">cast</a></h2>
<pre><code>cast(Expr, Expr)
</code></pre>
<p>Attempt to cast the second argument to the type specified by the
first. Produce a value of the specified type, or an error if the cast
is not possible.</p>
<p>e.g.</p>
<pre><code>cast(&quot;f32&quot;, load(&quot;/volume&quot;))
</code></pre>
<p>Changes volume into a single precision float if possible.</p>
<h2><a class="header" href="#isa" id="isa">isa</a></h2>
<pre><code>isa(Expr, Expr)
</code></pre>
<p>Produce true if the 2nd argument is the type named by the first
argument, false otherwise.</p>
<p>e.g.</p>
<pre><code>isa(&quot;f32&quot;, 10)
</code></pre>
<p>would produce false.</p>
<h2><a class="header" href="#eval" id="eval">eval</a></h2>
<pre><code>eval(Expr)
</code></pre>
<p>Compiles and executes the browser script program specified by it's
argument, or produces an error if the program is invalid.</p>
<p>e.g.</p>
<pre><code>eval(load(&quot;[base]/program&quot;))
</code></pre>
<p>Load and execute browser script from <code>[base]/program</code>.</p>
<h2><a class="header" href="#count" id="count">count</a></h2>
<pre><code>count(Expr)
</code></pre>
<p>Produces the count of events produced by expr since we started
execution of the pipeline.</p>
<p>e.g.</p>
<pre><code>count(load(&quot;/volume&quot;))
</code></pre>
<p>will increment every time volume changes.</p>
<h2><a class="header" href="#sample" id="sample">sample</a></h2>
<pre><code>sample(Expr, Expr)
</code></pre>
<p>Produces the value of it's second argument when it's first argument
updates.</p>
<p>e.g.</p>
<pre><code>sample(load(&quot;[base]/timestamp&quot;), load(&quot;[base]/voltage&quot;))
</code></pre>
<p>Produces <code>[base]/voltage</code> whenever <code>[base]/timestamp</code> updates.</p>
<h2><a class="header" href="#uniq" id="uniq">uniq</a></h2>
<pre><code>uniq(Expr)
</code></pre>
<p>Produces the value of it's argument only if that value is different
from the previous one.</p>
<p>e.g.</p>
<pre><code>uniq(load(&quot;[stock_base]/ibm/last&quot;))
</code></pre>
<p>Would produce an event only when the last trade price of IBM changes.</p>
<h2><a class="header" href="#string_join" id="string_join">string_join</a></h2>
<pre><code>string_join(sep: Expr, ..., Expr)
</code></pre>
<p>Concatinate all arguments from 2 ... n using the first argument as a
separator.</p>
<p>e.g.</p>
<pre><code>string_join(&quot;/&quot;, base, &quot;foo&quot;, &quot;bar&quot;)
</code></pre>
<p>is the same a writing <code>&quot;[base]/foo/bar&quot;</code></p>
<h2><a class="header" href="#string_concat" id="string_concat">string_concat</a></h2>
<pre><code>string_concat(Expr, ..., Expr)
</code></pre>
<p>Concatinate all arguments.</p>
<p>e.g.</p>
<pre><code>string_concat(load(&quot;/foo&quot;), load(&quot;/bar&quot;), &quot;baz&quot;)
</code></pre>
<p>is the same as writing <code>&quot;[load(&quot;/foo&quot;)][load(&quot;/bar&quot;)]baz&quot;</code>. And in
fact string interpolations are just syntactic sugar for this function.</p>
<h2><a class="header" href="#event" id="event">event</a></h2>
<pre><code>event()
</code></pre>
<p>Produces a widget specific event depending on which widget and which
event handler the pipeline containing it is attached to. For example,
when attached to an entry <code>on_change</code> handler it produces the string
value of the entry whenever the user changes the text. When attached
to the on_activate handler of the entry, it produces the string value
of the entry when the user presses the Enter key. When attached to the
<code>on_click</code> handler of a button, it produces Null every time the button
is clicked.</p>
<p>e.g.</p>
<pre><code>store(&quot;/text&quot;, event())
</code></pre>
<p>When attached to the <code>on_change</code> event of an entry would write the
text to <code>/text</code> every time the user changes it.</p>
<h2><a class="header" href="#confirm" id="confirm">confirm</a></h2>
<pre><code>confirm(msg: Expr, val: Expr)
</code></pre>
<p>Asks the user msg with val appended, and if they say yes produces it's
second argument, otherwise does not produce anything.</p>
<p>e.g.</p>
<pre><code>store(
  &quot;[base]/volume&quot;, 
  confirm(
    &quot;are you sure you want to change the volume to &quot;, 
    volume
  )
)
</code></pre>
<p>Asks the user to confirm before writing the value of the variable
<code>volume</code> to <code>[base]/volume</code>.</p>
<h2><a class="header" href="#navigate" id="navigate">navigate</a></h2>
<pre><code>navigate(Expr)
</code></pre>
<p>Navigate the browser to the location specified by it's first
argument. The syntax of a location is one of, </p>
<ul>
<li>a valid absolute netidx path, e.g. /foo/bar/baz</li>
<li>a view file e.g. file:/path/to/view/file</li>
<li>a netidx: prefixed netidx path, e.g. netidx:/foo/bar/baz</li>
</ul>
<p>e.g.</p>
<pre><code>navigate(confirm(&quot;go to &quot;, &quot;file:[next_view]&quot;))
</code></pre>
<h2><a class="header" href="#call" id="call">call</a></h2>
<pre><code>call(rpc: Expr, Expr, ..., Expr)
</code></pre>
<p>Call the netidx rpc specified by the first argument, passing the
specified keyword arguments, and producing the return value of the
call. Keyword arguments are encoded as pairs of a name followed by a
value.</p>
<p>e.g.</p>
<pre><code>store_var(
  &quot;sessionid&quot;,
  call(
    &quot;/solar/archive/session&quot;, 
    &quot;start&quot;, &quot;-10d&quot;, 
    &quot;speed&quot;, &quot;unlimited&quot;, 
    &quot;play_after&quot;, &quot;2s&quot;
  )
)
</code></pre>
<p>call <code>/solar/archive/session</code> with arguments to replay the last 10
days, starting 2 seconds after the call finishes, at unlimited speed,
and store the resulting session id in the variable sessionid.</p>
<h2><a class="header" href="#load_var" id="load_var">load_var</a></h2>
<pre><code>load_var(var: Expr)
var
</code></pre>
<p>Produce the value of the variable specified by var, or an error if var
is not a valid variable name. The second form is syntactic sugar that
translates into <code>load_var(&quot;var&quot;)</code>.</p>
<h2><a class="header" href="#store_var" id="store_var">store_var</a></h2>
<pre><code>store_var(name: Expr, val: Expr)
</code></pre>
<p>Store the value of val in the variable specified by name. Return
nothing, or an error if name is not a valid variable name.</p>
<p>e.g.</p>
<pre><code>store_var(&quot;volume&quot;, cast(&quot;f32&quot;, event()))
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
